{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from preconditioners import settings\n",
    "from preconditioners.datasets import generate_true_parameter, CenteredLinearGaussianDataset, generate_true_parameter, generate_c\n",
    "from preconditioners.utils import MLP, SLP\n",
    "from preconditioners.optimizers import PrecondGD\n",
    "import preconditioners\n",
    "import multiprocessing, threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in linear_kernel_variance_30102022.ipynb (and if it is not, it should be)\n",
    "\n",
    "from preconditioners.datasets import generate_c, generate_centered_linear_gaussian_data, generate_true_parameter, generate_W_star\n",
    "\n",
    "def kernel_variance_interpolator(features : np.ndarray, P : np.ndarray, F : np.ndarray, sigma2 : np.float64) -> np.float64:\n",
    "    ''' Given Nxp feature matrix {features}, pxp symmetric preconditioner P and pxp true covariance matrix F, and the signal to noise ratio, this function returns the variance component of the risk \n",
    "    of the interpolator which is the limit of PGD which uses preconditioner P.'''\n",
    "    assert features.shape[1] == P.shape[0] == P.shape[1] == F.shape[0] == F.shape[1]\n",
    "    assert abs(P - P.T).mean() < 1e-4, 'P must be symmetric.'\n",
    "\n",
    "    empirical_NTK = features.dot(P).dot(features.T)\n",
    "    empirical_NTK_inv = np.linalg.inv(empirical_NTK)\n",
    "    auxi_matrix = features.dot(P).dot(F).dot(P).dot(features.T)\n",
    "\n",
    "    return sigma2 * np.trace(empirical_NTK_inv.dot(auxi_matrix).dot(empirical_NTK_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted CheckEigenvalues class from src/preconditioners/eigenvalues/kernel_eigenvalues.py\n",
    "# with the linear gaussian dataset\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self,\n",
    "                width,\n",
    "                depth,\n",
    "                d,\n",
    "                #lam,\n",
    "                train_size,\n",
    "                extra_size,\n",
    "                sigma2,\n",
    "                r2,\n",
    "                regime,\n",
    "                ro,\n",
    "                ):\n",
    "        # Network parameters\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.d = d\n",
    "        #self.damping = lam * np.sqrt(self.d) if self.depth == 1 else lam * self.width\n",
    "        self.damping = 0\n",
    "\n",
    "        # Dataset parameters\n",
    "        self.train_size = train_size\n",
    "        self.extra_size = extra_size\n",
    "\n",
    "        w_star = generate_true_parameter(self.d, r2, m=np.eye(self.d))\n",
    "        c = generate_c(ro, regime=regime, d=self.d) if self.depth > 1 else generate_c(ro, regime=regime, d=self.d) * np.sqrt(self.d) # we don't need to multiply by sqrt(d) if d > 1 because there c is not the Fisher\n",
    "        dataset = CenteredLinearGaussianDataset(w_star, c, n=self.train_size + self.extra_size, d=self.d, sigma2=sigma2)\n",
    "\n",
    "        self.train_dataset, self.extra_dataset = random_split(dataset, [self.train_size, self.extra_size])\n",
    "        self.labeled_data = self.train_dataset[:][0].double().to(settings.DEVICE)\n",
    "        self.unlabeled_data = self.extra_dataset[:][0].double().to(settings.DEVICE)\n",
    "\n",
    "    def create_model(self):\n",
    "        std = 1\n",
    "        #std = 1/ np.sqrt(self.width)\n",
    "\n",
    "        # Create model and optimizer\n",
    "        if self.depth == 1:\n",
    "            self.model = SLP(in_channels=self.d).double().to(settings.DEVICE)\n",
    "        else:\n",
    "            self.model = MLP(in_channels=self.d, num_layers=self.depth, hidden_channels=self.width, std=std).double().to(settings.DEVICE)\n",
    "        self.optimizer = PrecondGD(self.model, lr=1e-2, labeled_data=self.labeled_data, unlabeled_data=self.unlabeled_data, verbose=False, damping=self.damping)\n",
    "\n",
    "    def get_features_and_p_inv(self):\n",
    "         self.create_model()\n",
    "         p_inv = self.optimizer._compute_p_inv()\n",
    "         grad = self.optimizer._compute_grad_of_data(self.labeled_data)\n",
    "         return grad, p_inv\n",
    "\n",
    "    def get_features_and_F(self):\n",
    "        self.create_model()\n",
    "        F = self.optimizer._compute_fisher()\n",
    "        grad = self.optimizer._compute_grad_of_data(self.labeled_data)\n",
    "        return grad, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/src/preconditioners/datasets.py:153: UserWarning: Warning, norms of datapoints are not sqrt(d)\n",
      "  warnings.warn('Warning, norms of datapoints are not sqrt(d)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0441, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## Test that I get the same results if the MLP is a linear network.\n",
    "\n",
    "N = 20\n",
    "N_extra = 30000\n",
    "sigma2 = 2\n",
    "ro = 0.9\n",
    "n_ds = 10\n",
    "n_lams = 10\n",
    "ds = [int(x) for x in np.linspace(2*N,4*N,n_ds)]\n",
    "ams = np.linspace(0.1,10,n_lams)\n",
    "lams = np.logspace(0, -3, num=n_lams, endpoint=True, base=10.0)\n",
    "\n",
    "d = 100\n",
    "lam = 0.1\n",
    "\n",
    "# linear\n",
    "np.random.seed(0)\n",
    "damping = lam * np.sqrt(d)\n",
    "c = np.sqrt(d) * generate_c(ro=ro, regime='autoregressive', d=d)\n",
    "X = np.random.multivariate_normal(mean=np.zeros(d), cov=c, size=N)\n",
    "#P_damped = np.linalg.inv(c + damping*np.eye(d))\n",
    "P_ngd = np.linalg.inv(c)\n",
    "\n",
    "# get it through the initializer\n",
    "i = Initializer(width = 1,\n",
    "                depth = 1,\n",
    "                d = d,\n",
    "                lam = lam,\n",
    "                train_size = N,\n",
    "                extra_size = N_extra,\n",
    "                sigma2 = sigma2,\n",
    "                r2 = 1,\n",
    "                regime = 'autoregressive',\n",
    "                ro = ro)\n",
    "\n",
    "grad, F = i.get_features_and_F()\n",
    "#grad, F_inv = i.get_features_and_p_inv()\n",
    "\n",
    "print(abs(F-c).mean())\n",
    "#print(np.linalg.norm(F_inv - P_damped)/np.sqrt(F_inv.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0206,  9.0127,  8.1454],\n",
       "        [ 9.0127,  9.9688,  8.9998],\n",
       "        [ 8.1454,  8.9998, 10.0088]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10. ,  9. ,  8.1],\n",
       "       [ 9. , 10. ,  9. ],\n",
       "       [ 8.1,  9. , 10. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "\n",
    "# In the below, am I not changing the dimension of the data, instead of changing the dimension of the model?\n",
    "# Fix it.\n",
    "\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.721999263637377e-16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(P_ngd - P_ngd.T).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.46415888, 0.21544347, 0.1       , 0.04641589,\n",
       "       0.02154435, 0.01      , 0.00464159, 0.00215443, 0.001     ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcessMainProcess\n",
      "\n",
      "MainProcess\n",
      "MainProcess\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.0001\n",
      "MainProcess\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.0001\n",
      "MainProcess\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.0001\n",
      "MainProcess\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.0001\n",
      "MainProcess\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.0001\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.0001\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 0.0001\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 0.0001\n",
      "[(0, 0, 3.407172621242778, 0.4788112539485314), (0, 1, 1.3974985210405713, 0.4788112539485314), (0, 2, 0.04395809955455232, 0.4788112539485314), (1, 0, 3.6017422656341047, 0.4750489401401466), (1, 1, 1.408812098089194, 0.4750489401401466), (1, 2, 0.10722190395456166, 0.4750489401401466), (2, 0, 2.576375831431649, 0.4145182547938), (2, 1, 1.4442478981879479, 0.4145182547938), (2, 2, 0.15083038403568855, 0.4145182547938), (3, 0, 1.7878651437457993, 0.42641816650253006), (3, 1, 0.7081254824085409, 0.42641816650253006), (3, 2, 0.0768285780437169, 0.42641816650253006), (4, 0, 3.3041346903624382, 0.1499528114236472), (4, 1, 2.709192313284739, 0.1499528114236472), (4, 2, 0.5130035916582859, 0.1499528114236472), (5, 0, 1.6923293766214518, 0.2337295616456972), (5, 1, 0.9933353603965738, 0.2337295616456972), (5, 2, 0.0871108940356573, 0.2337295616456972), (6, 0, 1.7632844247373183, 0.23296560090964466), (6, 1, 1.1942415434782248, 0.23296560090964466), (6, 2, 0.32148079210566716, 0.23296560090964466), (7, 0, 2.2742174275406537, 0.18836569214982543), (7, 1, 1.456387931540271, 0.18836569214982543), (7, 2, 0.46051099701607806, 0.18836569214982543)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "N = 10\n",
    "N_extra = 30000\n",
    "d = 15\n",
    "sigma2 = 2\n",
    "ro = 0.9\n",
    "n_ns = 8\n",
    "n_lams = 3\n",
    "variances_damped = np.zeros((n_ns,n_lams))\n",
    "variances_ngd = np.zeros((n_ns,n_lams))\n",
    "variances_diff = np.zeros((n_ns,n_lams))    \n",
    "largest_eigval_F_div_n = np.zeros((n_ns,n_lams))\n",
    "\n",
    "ns = [int(x) for x in np.linspace(int(np.sqrt(1.5*N)),int(np.sqrt(6*N)),n_ns)]\n",
    "ps = np.zeros(n_ns)\n",
    "#lams = np.logspace(0, -5, num=n_lams, endpoint=True, base=10.0)\n",
    "lams = [1.0, 0.01, 0.0001]\n",
    "dampings = []\n",
    "\n",
    "# save results as tuples (i, j, variances_diff, largest_eigval_F_div_n)\n",
    "results = []\n",
    "def get_variances_n(i: int, d, N, N_extra, sigma2, ro, lams, ns):\n",
    "    # print current cpu id\n",
    "    process = multiprocessing.current_process()\n",
    "    print(process.name)\n",
    "    res = []\n",
    "    n = ns[i]\n",
    "    params = {\n",
    "            'width' : n,\n",
    "            'depth' : 2,\n",
    "            'd' : d,\n",
    "            'lam' : -10000000, # damping does not matter here if we are not using .get_features_and_p_inv()\n",
    "            'train_size' : N,\n",
    "            'extra_size' : N_extra,\n",
    "            'sigma2' : sigma2,\n",
    "            'r2' : 1,\n",
    "            'regime' : 'autoregressive',\n",
    "            'ro' : ro\n",
    "        }\n",
    "    ini = Initializer(**params)\n",
    "    features, F_extra = [x.numpy() for x in ini.get_features_and_F()]\n",
    "    p = F_extra.shape[0]\n",
    "    P_ngd = np.linalg.inv(F_extra)\n",
    "\n",
    "    ### TODO: There is an error here!! this eigenvalue (divided by n) is decreasing with n, but it should be constant.\n",
    "    ### check if I'm doing the right parametrization!!!\n",
    "    \n",
    "    e_F_div_n = np.linalg.eigvalsh(F_extra)[-1]/n\n",
    "    v_ngd = kernel_variance_interpolator(features=features, P=P_ngd, F=F_extra, sigma2=sigma2)    \n",
    "    \n",
    "    for j in range(n_lams):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        lam = lams[j]\n",
    "        damping = lam * n\n",
    "        print(f'p = {p}')\n",
    "        print(f'lambda = {lam}')\n",
    "\n",
    "        P_damped = np.linalg.inv(F_extra + damping * np.eye(p))\n",
    "\n",
    "        #largest_eigval_F_div_n[i, j] = e_F_div_n\n",
    "        v_damped = kernel_variance_interpolator(features=features, P=P_damped, F=F_extra, sigma2=sigma2) \n",
    "        v_diff = v_damped - v_ngd\n",
    "        res.append((i, j, v_diff, e_F_div_n))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# for i in range(n_ns):\n",
    "#     results += get_variances_n(i, d, N, N_extra, sigma2, ro, lams, ns)\n",
    "\n",
    "from functools import partial\n",
    "get_variances_n_fixed = partial(get_variances_n, d=d, N=N, N_extra=N_extra, sigma2=sigma2, ro=ro, lams=lams, ns=ns)\n",
    "\n",
    "with ThreadPool(processes=4) as pool:\n",
    "    for res in pool.map(get_variances_n_fixed, [i for i in range(len(ns))]):\n",
    "        results.extend(res)\n",
    "\n",
    "# use multiprocessing to run the above function in parallel for each n in ns and save them intor results\n",
    "# if __name__ == '__main__':\n",
    "#     with multiprocessing.Pool(processes=4) as pool:\n",
    "#         for i, res in enumerate(pool.starmap(get_variances_n, [(i, d, N, N_extra, sigma2, ro, lams, ns) for i in range(len(ns))])):\n",
    "#             results += res\n",
    "\n",
    "print(results)\n",
    "\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     results = list(executor.map(get_variances_n, range(n_ns)))\n",
    "\n",
    "# processes = []\n",
    "# for i in range(n_ns):\n",
    "#     p = multiprocessing.Process(target=get_variances_n, args=(i,))\n",
    "#     processes.append(p)\n",
    "#     p.start()\n",
    "\n",
    "# for p in processes:\n",
    "#     p.join()\n",
    "\n",
    "# threads = []\n",
    "# for i in range(n_ns):\n",
    "#     t = threading.Thread(target=get_variances_n, args=(i,))\n",
    "#     threads.append(t)\n",
    "#     t.start()\n",
    "\n",
    "# for t in threads:\n",
    "#     t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variances_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/src/preconditioners/datasets.py:153: UserWarning: Warning, norms of datapoints are not sqrt(d)\n",
      "  warnings.warn('Warning, norms of datapoints are not sqrt(d)')\n",
      "/usr/local/Caskroom/miniforge/base/envs/preconditioners/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.739849090576172\n",
      "0.0004298686981201172\n",
      " \n",
      "7\n",
      "19.815614938735962\n",
      "0.004244089126586914\n",
      " \n",
      "11\n",
      "24.4473819732666\n",
      "0.0038690567016601562\n",
      " \n",
      "15\n",
      "33.920655965805054\n",
      "0.020847082138061523\n",
      " \n",
      "19\n",
      "37.280600786209106\n",
      "0.006659030914306641\n",
      " \n",
      "23\n",
      "45.936105251312256\n",
      "0.01545405387878418\n",
      " \n",
      "27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ini \u001b[39m=\u001b[39m Initializer(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m features, F_extra \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ini\u001b[39m.\u001b[39;49mget_features_and_F()]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(t2\u001b[39m-\u001b[39mt1)\n",
      "\u001b[1;32m/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb Cell 15\u001b[0m in \u001b[0;36mInitializer.get_features_and_F\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_features_and_F\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_model()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     F \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49m_compute_fisher()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39m_compute_grad_of_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabeled_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Desktop/ml_research/phd/preconditioners/notebooks/02042023_faster_NTK_kernel_variance.ipynb#X62sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grad, F\n",
      "File \u001b[0;32m~/Desktop/ml_research/phd/preconditioners/src/preconditioners/optimizers/precond.py:28\u001b[0m, in \u001b[0;36mPrecondGD._compute_fisher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m unlabeled_data:\n\u001b[0;32m---> 28\u001b[0m     grad \u001b[39m=\u001b[39m model_gradients_using_backprop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, x)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     29\u001b[0m     p \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grad \u001b[39m@\u001b[39m grad\u001b[39m.\u001b[39mT\n\u001b[1;32m     30\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ml_research/phd/preconditioners/src/preconditioners/utils.py:319\u001b[0m, in \u001b[0;36mmodel_gradients_using_backprop\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39m\"\"\"Computes model gradients -- used in precond.py\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 319\u001b[0m get_jacobian(model, x, noutputs\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mout_dim)\n\u001b[1;32m    320\u001b[0m grads \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters()])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    321\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/ml_research/phd/preconditioners/src/preconditioners/utils.py:312\u001b[0m, in \u001b[0;36mget_jacobian\u001b[0;34m(net, x, noutputs)\u001b[0m\n\u001b[1;32m    310\u001b[0m x\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    311\u001b[0m y \u001b[39m=\u001b[39m net(x)\n\u001b[0;32m--> 312\u001b[0m y\u001b[39m.\u001b[39mbackward(torch\u001b[39m.\u001b[39;49meye(noutputs)\u001b[39m.\u001b[39mto(DEVICE))\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evals = []\n",
    "import time\n",
    "for n in [3,7,11,15,19,23,27]:\n",
    "    print(n)\n",
    "    params = {\n",
    "            'width' : n,\n",
    "            'depth' : 2,\n",
    "            'd' : 15,\n",
    "            #'lam' : -10000000, # damping does not matter here if we are not using .get_features_and_p_inv()\n",
    "            'train_size' : 10,\n",
    "            'extra_size' : 200000,\n",
    "            'sigma2' : 2,\n",
    "            'r2' : 1,\n",
    "            'regime' : 'autoregressive',\n",
    "            'ro' : 0.7\n",
    "        }\n",
    "    ini = Initializer(**params)\n",
    "    t1 = time.time()\n",
    "    features, F_extra = [x.numpy() for x in ini.get_features_and_F()]\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    evals.append(np.linalg.eigvalsh(F_extra)[-1])\n",
    "    t3 = time.time()\n",
    "    print(t3-t2)\n",
    "    print(' ')\n",
    "\n",
    "print(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6141933705994282, 2.8062662966382836, 2.5660470814662726, 4.794735439470179, 4.852510064231936, 5.511911164822392]\n"
     ]
    }
   ],
   "source": [
    "print(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 391)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.511911164822399"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(F_extra)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "iteration 1 of 18\n",
      "p = 51\n",
      "damping = 3.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 2 of 18\n",
      "p = 51\n",
      "damping = 0.03\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 3 of 18\n",
      "p = 51\n",
      "damping = 0.00030000000000000003\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 4 of 18\n",
      "p = 85\n",
      "damping = 5.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 5 of 18\n",
      "p = 85\n",
      "damping = 0.05\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 6 of 18\n",
      "p = 85\n",
      "damping = 0.0005\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 7 of 18\n",
      "p = 119\n",
      "damping = 7.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 8 of 18\n",
      "p = 119\n",
      "damping = 0.07\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 9 of 18\n",
      "p = 119\n",
      "damping = 0.0007\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 10 of 18\n",
      "p = 153\n",
      "damping = 9.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 11 of 18\n",
      "p = 153\n",
      "damping = 0.09\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 12 of 18\n",
      "p = 153\n",
      "damping = 0.0009000000000000001\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 13 of 18\n",
      "p = 187\n",
      "damping = 11.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 14 of 18\n",
      "p = 187\n",
      "damping = 0.11\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 15 of 18\n",
      "p = 187\n",
      "damping = 0.0011\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 16 of 18\n",
      "p = 221\n",
      "damping = 13.0\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 17 of 18\n",
      "p = 221\n",
      "damping = 0.13\n",
      " \n",
      "--------------------------------------------------\n",
      "iteration 18 of 18\n",
      "p = 221\n",
      "damping = 0.0013000000000000002\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "N_extra = 200000\n",
    "d = 15\n",
    "sigma2 = 2\n",
    "ro = 0.9\n",
    "#n_ns = 5\n",
    "#n_lams = 1\n",
    "#ns = [int(x) for x in np.linspace(int(np.sqrt(1.5*N)),int(np.sqrt(8*N)),n_ns)]\n",
    "ns = [3,5,7,9,11,13]\n",
    "n_ns = len(ns)\n",
    "ps = []\n",
    "#lams = np.logspace(0, -5, num=n_lams, endpoint=True, base=10.0)\n",
    "lams = [1.0, 0.01, 0.0001]\n",
    "n_lams = len(lams)\n",
    "#lams = [1.0]\n",
    "variances_damped = np.zeros((n_ns,n_lams))\n",
    "variances_ngd = np.zeros((n_ns,n_lams))\n",
    "variances_diff = np.zeros((n_ns,n_lams))    \n",
    "largest_eigval_F_div_n = np.zeros((n_ns,n_lams))\n",
    "dampings = []\n",
    "\n",
    "k = 0\n",
    "for i in range(n_ns):\n",
    "    n = ns[i]\n",
    "    params = {\n",
    "            'width' : n,\n",
    "            'depth' : 2,\n",
    "            'd' : d,\n",
    "            #'lam' : -10000000, # damping does not matter here if we are not using .get_features_and_p_inv()\n",
    "            'train_size' : N,\n",
    "            'extra_size' : N_extra,\n",
    "            'sigma2' : sigma2,\n",
    "            'r2' : 1,\n",
    "            'regime' : 'autoregressive',\n",
    "            'ro' : ro\n",
    "        }\n",
    "    ini = Initializer(**params)\n",
    "    features, F_extra = [x.numpy() for x in ini.get_features_and_F()]\n",
    "    p = F_extra.shape[0]\n",
    "    ps.append(p)\n",
    "    P_ngd = np.linalg.inv(F_extra)\n",
    "\n",
    "    ### TODO: There is an error here!! this eigenvalue (divided by n) is decreasing with n, but it should be constant.\n",
    "    ### check if I'm doing the right parametrization!!!\n",
    "\n",
    "    \n",
    "    e_F_div_n = np.linalg.eigvalsh(F_extra)[-1]#/n\n",
    "    v_ngd = kernel_variance_interpolator(features=features, P=P_ngd, F=F_extra, sigma2=sigma2)\n",
    "\n",
    "    for j in range(n_lams):\n",
    "        k = k+1\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'iteration {k} of {n_ns*n_lams}')\n",
    "        print(f'p = {p}')\n",
    "        lam = lams[j]\n",
    "        damping = lam * n\n",
    "        dampings.append(damping)\n",
    "        print(f'damping = {damping}')\n",
    "        \n",
    "        P_damped = np.linalg.inv(F_extra + damping * np.eye(p))\n",
    "        \n",
    "        largest_eigval_F_div_n[i, j] = e_F_div_n\n",
    "        variances_damped[i, j] = kernel_variance_interpolator(features=features, P=P_damped, F=F_extra, sigma2=sigma2) \n",
    "        variances_ngd[i, j] = v_ngd\n",
    "        variances_diff = variances_damped - variances_ngd\n",
    "        variances_diff[i, j] = variances_damped[i, j] - variances_ngd[i, j]\n",
    "\n",
    "# choose only unique ps\n",
    "ps = np.unique(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 9, 11, 13]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.01, 0.0001]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 221)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_ngd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.244205"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p**2 / N_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.233401119390962e-10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(P_ngd - P_ngd.T).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7, 9, 11, 13]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.01, 0.0001]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(variances_diff, cmap='viridis', xticklabels=[round(x, 5) for x in lams], yticklabels=np.unique(ps), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide each row of largest_eigval_F_div_n by the corresponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(largest_eigval_F_div_n, cmap='viridis', xticklabels=[round(x, 1) for x in lams], yticklabels=ns, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Triple check that we are computing the inverse well now and that we were doing it wrong before. Because the tests were passing before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.07106781, 6.36396103, 5.72756493],\n",
       "       [6.36396103, 7.07106781, 6.36396103],\n",
       "       [5.72756493, 6.36396103, 7.07106781]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.77817459, 6.36396103, 5.72756493],\n",
       "       [6.36396103, 7.77817459, 6.36396103],\n",
       "       [5.72756493, 6.36396103, 7.77817459]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c + damping*np.eye(d))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40672876, -0.26077797, -0.06749992],\n",
       "       [-0.26077797,  0.573929  , -0.21749976],\n",
       "       [-0.06749992, -0.21749976,  0.58513116]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(c+ damping*np.eye(d))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0352, -0.0173, -0.0002],\n",
       "        [-0.0173,  0.0352, -0.0173],\n",
       "        [-0.0002, -0.0173,  0.0352]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cholesky_inverse(torch.from_numpy(c+ damping*np.eye(d)))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1522, -0.1304, -0.0057],\n",
       "        [-0.1304,  0.2640, -0.1256],\n",
       "        [-0.0057, -0.1256,  0.2642]], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cholesky_inverse(torch.cholesky(torch.from_numpy(c+ damping*np.eye(d))))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15224482, -0.13042363, -0.00565121],\n",
       "       [-0.13042363,  0.26397487, -0.1255824 ],\n",
       "       [-0.00565121, -0.1255824 ,  0.26418464]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(c+ damping*np.eye(d))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15224482, -0.13042363, -0.00565121],\n",
       "       [-0.13042363,  0.26397487, -0.1255824 ],\n",
       "       [-0.00565121, -0.1255824 ,  0.26418464]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_damped[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73584905e-03, -8.49996340e-04, -8.67206609e-06],\n",
       "       [-8.49996340e-04,  1.73288370e-03, -8.62042779e-04],\n",
       "       [-8.67206609e-06, -8.62042779e-04,  1.76788253e-03]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_inv.numpy()[:3,:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Lambda parralel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 1.0\n",
      "\n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 51\n",
      "lambda = 0.0001\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 1.0 \n",
      "\n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 68\n",
      "lambda = 0.0001\n",
      "  \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 1.0\n",
      "\n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.01\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 85\n",
      "lambda = 0.0001\n",
      "  \n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 1.0\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 0.0001\n",
      "\n",
      "--------------------------------------------------\n",
      "p = 102\n",
      "lambda = 0.01\n",
      "  \n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 1.0\n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 0.01\n",
      "\n",
      " \n",
      "--------------------------------------------------\n",
      "p = 119\n",
      "lambda = 0.0001\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "N_extra = 30000\n",
    "d = 15\n",
    "sigma2 = 2\n",
    "ro = 0.9\n",
    "n_ns = 5\n",
    "n_lams = 3\n",
    "variances_damped = np.zeros((n_ns,n_lams))\n",
    "variances_ngd = np.zeros((n_ns,n_lams))\n",
    "variances_diff = np.zeros((n_ns,n_lams))    \n",
    "largest_eigval_F_div_n = np.zeros((n_ns,n_lams))\n",
    "\n",
    "ns = [int(x) for x in np.linspace(int(np.sqrt(1.5*N)),int(np.sqrt(6*N)),n_ns)]\n",
    "ps = []\n",
    "#lams = np.logspace(0, -5, num=n_lams, endpoint=True, base=10.0)\n",
    "lams = [1.0, 0.01, 0.0001]\n",
    "dampings = []\n",
    "\n",
    "def get_variances(i: int, j: int, n: int, p: int, F_extra: np.ndarray, features: np.ndarray, sigma2: float, e_F_div_n: float, v_ngd: float):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        lam = lams[j]\n",
    "        damping = lam * n\n",
    "        print(f'p = {p}')\n",
    "        print(f'lambda = {lam}')\n",
    "\n",
    "        P_damped = np.linalg.inv(F_extra + damping * np.eye(p))\n",
    "\n",
    "        largest_eigval_F_div_n[i, j] = e_F_div_n\n",
    "        variances_damped[i, j] = kernel_variance_interpolator(features=features, P=P_damped, F=F_extra, sigma2=sigma2) \n",
    "        variances_ngd[i, j] = v_ngd\n",
    "        variances_diff[i, j] = variances_damped[i, j] - variances_ngd[i, j]\n",
    "\n",
    "for i in range(n_ns):\n",
    "    n = ns[i]\n",
    "    params = {\n",
    "            'width' : n,\n",
    "            'depth' : 2,\n",
    "            'd' : d,\n",
    "            'lam' : -10000000, # damping does not matter here if we are not using .get_features_and_p_inv()\n",
    "            'train_size' : N,\n",
    "            'extra_size' : N_extra,\n",
    "            'sigma2' : sigma2,\n",
    "            'r2' : 1,\n",
    "            'regime' : 'autoregressive',\n",
    "            'ro' : ro\n",
    "        }\n",
    "    ini = Initializer(**params)\n",
    "    features, F_extra = [x.numpy() for x in ini.get_features_and_F()]\n",
    "    p = F_extra.shape[0]\n",
    "    ps.append(p)\n",
    "    P_ngd = np.linalg.inv(F_extra)\n",
    "\n",
    "    ### TODO: There is an error here!! this eigenvalue (divided by n) is decreasing with n, but it should be constant.\n",
    "    ### check if I'm doing the right parametrization!!!\n",
    "    \n",
    "    e_F_div_n = np.linalg.eigvalsh(F_extra)[-1]/n\n",
    "    v_ngd = kernel_variance_interpolator(features=features, P=P_ngd, F=F_extra, sigma2=sigma2)\n",
    "\n",
    "    threads = []\n",
    "    for j in range(n_lams):\n",
    "        t = threading.Thread(target=get_variances, args=(i, j, n, p, F_extra, features, sigma2, e_F_div_n, v_ngd,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "# choose only unique ps\n",
    "ps = np.unique(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preconditioners",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c2d541a5db7c05910ac06554e3a79dfe5bead4166f54a2f5ce9b07705345cc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
