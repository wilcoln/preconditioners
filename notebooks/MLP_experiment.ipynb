{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from preconditioners.datasets import CenteredGaussianDataset\n",
    "from preconditioners.optimizers import PrecondGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardoravkin/Dropbox/My Mac (Eduardâ€™s MacBook Pro)/Desktop/ml_research/PhD/preconditioners/notebooks/../preconditioners/utils.py:219: UserWarning: Warning, norms of datapoints are not sqrt(d)\n",
      "  warnings.warn('Warning, norms of datapoints are not sqrt(d)')\n"
     ]
    }
   ],
   "source": [
    "dataset = CenteredGaussianDataset(w_star=np.ones(2), d = 2, c=np.ones((2, 2)), n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "      Multilayer Perceptron for regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "#             nn.Linear(in_size, 1),\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "          Forward pass\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep the following layers in PrecondGD. \n",
      "(0): Linear(in_features=2, out_features=64, bias=True)\n",
      "(1): Linear(in_features=64, out_features=32, bias=True)\n",
      "(2): Linear(in_features=32, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCHS = 100\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(0)\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create trainloader object\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "# Create testloader object\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(in_size=dataset.X.shape[1])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "num_params = sum(p.numel() for p in mlp.parameters())\n",
    "# p_inv = np.zeros((num_params, num_params))\n",
    "p_inv = np.eye(num_params)\n",
    "optimizer = PrecondGD(mlp, lr=1e-4, p_inv=p_inv)\n",
    "\n",
    "\n",
    "losses = {'train': [], 'test': []}\n",
    "epochs = range(NB_EPOCHS)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "     # Print epoch\n",
    "#     print(f'Starting epoch {epoch + 1}')\n",
    "\n",
    "    mlp.train()\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(trainloader)\n",
    "    \n",
    "    losses['train'].append(current_loss)\n",
    "    \n",
    "#     print('Train Loss: %.3f'%(current_loss))\n",
    "\n",
    "    \n",
    "def test(epoch):\n",
    "    mlp.eval()\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(testloader)\n",
    "    \n",
    "    losses['test'].append(current_loss)\n",
    "    \n",
    "#     print('Test Loss: %.3f'%(current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "for epoch in epochs:\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses['train'], '-o')\n",
    "plt.plot(losses['test'], '-o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Losses')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d8cd6e4c5e5d135b57c7f4afa48b47bbab254644d54890f770c6527a3ab3f54"
  },
  "kernelspec": {
   "display_name": "bias-nlp-mt21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
