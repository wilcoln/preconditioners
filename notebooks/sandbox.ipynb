{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from preconditioners.datasets import CenteredGaussianDataset\n",
    "from preconditioners.optimizers import PrecondGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilcoln/Workspace/Projects/preconditioners/notebooks/../src/preconditioners/utils.py:219: UserWarning: Warning, norms of datapoints are not sqrt(d)\n",
      "  warnings.warn('Warning, norms of datapoints are not sqrt(d)')\n"
     ]
    }
   ],
   "source": [
    "dataset = CenteredGaussianDataset(w_star=np.ones(2), d = 2, c=np.ones((2, 2)), n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "      Multilayer Perceptron for regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "#             nn.Linear(in_size, 1),\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "          Forward pass\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "=> We keep the following layers in PrecondGD. \n",
      "(0): Linear(in_features=2, out_features=64, bias=True)\n",
      "(1): Linear(in_features=64, out_features=32, bias=True)\n",
      "(2): Linear(in_features=32, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCHS = 2\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(0)\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "extra_size = len(dataset) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, extra_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, extra_size])\n",
    "\n",
    "# Create trainloader object\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "# Create testloader object\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(in_size=dataset.X.shape[1])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "# num_params = sum(p.numel() for p in mlp.parameters())\n",
    "# p_inv = np.zeros((num_params, num_params))\n",
    "# p_inv = np.eye(num_params)\n",
    "optimizer = PrecondGD(mlp, lr=1e-4, labeled_data=train_dataset[:][0], unlabeled_data=extra_dataset[:][0])\n",
    "\n",
    "\n",
    "losses = {'train': [], 'test': []}\n",
    "epochs = range(NB_EPOCHS)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "     # Print epoch\n",
    "#     print(f'Starting epoch {epoch + 1}')\n",
    "\n",
    "    mlp.train()\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(trainloader)\n",
    "    \n",
    "    losses['train'].append(current_loss)\n",
    "    \n",
    "#     print('Train Loss: %.3f'%(current_loss))\n",
    "\n",
    "    \n",
    "def test(epoch):\n",
    "    mlp.eval()\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(testloader)\n",
    "    \n",
    "    losses['test'].append(current_loss)\n",
    "    \n",
    "#     print('Test Loss: %.3f'%(current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_17165/637214923.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Run the training loop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_17165/2819722678.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(epoch)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0;31m# Perform optimization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;31m# Print statistics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/bias-nlp-mt21/lib/python3.9/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspace/Projects/preconditioners/notebooks/../src/preconditioners/optimizers.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    150\u001B[0m         \u001B[0mp_grad_mat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_matrix_form_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mm\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    151\u001B[0m         \u001B[0mp_grad_vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_vector_form_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp_grad_mat\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mm\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 152\u001B[0;31m         \u001B[0mp_inv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_p_inv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Workspace/Projects/preconditioners/notebooks/../src/preconditioners/optimizers.py\u001B[0m in \u001B[0;36m_compute_p_inv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    174\u001B[0m         \u001B[0my_unlabeled\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequires_grad\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 176\u001B[0;31m         \u001B[0mlabeled_grads\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my_labeled\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabeled_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_labeled\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    177\u001B[0m         \u001B[0munlabeled_grads\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my_unlabeled\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0munlabeled_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_unlabeled\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/bias-nlp-mt21/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mgrad\u001B[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001B[0m\n\u001B[1;32m    232\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 234\u001B[0;31m     return Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    235\u001B[0m         \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_outputs_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    236\u001B[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in epochs:\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses['train'], '-o')\n",
    "plt.plot(losses['test'], '-o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Losses')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad \n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "y = (x+1) ** 2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "dy_dx = grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(f'dy/dx:\\n {dy_dx}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias-nlp-mt21",
   "language": "python",
   "name": "bias-nlp-mt21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}