{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try the variance approx with \\Sigma = np.linalg.inv(glasso) and plug it directly into the graph to check out whether it does better than the glasso.\n",
    "\n",
    "- It converges nicely. Need to find a way how to devise a method to test the perormance of these new estimators empirically beacuse this takes ages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from preconditioners.utils import generate_c, generate_centered_gaussian_data\n",
    "from preconditioners.cov_approx.variance_cov_approx import *\n",
    "from sklearn.covariance import  GraphicalLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the run\n",
    "n_epochs = 100\n",
    "iter_per_epoch = 500\n",
    "tol = 0.0001\n",
    "n = 100\n",
    "d = 300\n",
    "sigma2 = 1\n",
    "ro = 0.5\n",
    "regime = 'autoregressive'\n",
    "regul_lambda = 0 # for log barrier use 0.001\n",
    "lr_start = 0.5\n",
    "lr_decay = 0.98\n",
    "\n",
    "params = {\n",
    "    'n_epochs' : n_epochs,\n",
    "    'iter_per_epoch' : iter_per_epoch,\n",
    "    'tol' : tol,\n",
    "    'n' : n,\n",
    "    'd' : d,\n",
    "    'sigma2' : sigma2,\n",
    "    'ro' : ro,\n",
    "    'regime' : regime,\n",
    "    'regul_lambda' : regul_lambda,\n",
    "    'lr_start' : lr_start,\n",
    "    'lr_decay' : lr_decay\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardoravkin/Dropbox/My Mac (Eduardâ€™s MacBook Pro)/Desktop/ml_research/PhD/preconditioners/src/preconditioners/utils.py:219: UserWarning: Warning, norms of datapoints are not sqrt(d)\n",
      "  warnings.warn('Warning, norms of datapoints are not sqrt(d)')\n"
     ]
    }
   ],
   "source": [
    "# generate data and initialization\n",
    "c = generate_c(ro=ro,\n",
    "                regime=regime,\n",
    "                n=n,\n",
    "                d=d\n",
    "                )\n",
    "w_star = np.random.multivariate_normal(mean=np.zeros(d), cov=np.eye(d))\n",
    "X, y, xi = generate_centered_gaussian_data(w_star,\n",
    "                                            c,\n",
    "                                            n=n,\n",
    "                                            d=d,\n",
    "                                            sigma2=sigma2,\n",
    "                                            fix_norm_of_x=False)\n",
    "\n",
    "# initialize C (cholesky), cov_inv, regul_lambda and learning rate\n",
    "# is this a good way to initialize?\n",
    "cov_empir = X.T.dot(X) / n\n",
    "cov_inv = np.linalg.inv(cov_empir + 0.1 * np.eye(d))\n",
    "gl = GraphicalLasso(assume_centered=True, alpha=0.25, tol=1e-4).fit(X)\n",
    "cov_gl = gl.covariance_\n",
    "C = scipy.linalg.cholesky(cov_inv) + 0.5 * generate_c(ro=0.2,\n",
    "                                                        regime='autoregressive',\n",
    "                                                        n=n,\n",
    "                                                        d=d,\n",
    "                                                        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0/500 of epoch 0/100, loss 1.2356784562484702 and error 0.14041451922415543\n",
      "iteration 50/500 of epoch 0/100, loss 0.9754877248832658 and error 0.07544548437373999\n",
      "iteration 100/500 of epoch 0/100, loss 0.8745091490209388 and error 0.053543012752988194\n",
      "iteration 150/500 of epoch 0/100, loss 0.8183241990435243 and error 0.041932059866995275\n",
      "iteration 200/500 of epoch 0/100, loss 0.7819627053466467 and error 0.034649445833522294\n",
      "iteration 250/500 of epoch 0/100, loss 0.7562722378283997 and error 0.02962794819679009\n",
      "iteration 300/500 of epoch 0/100, loss 0.7370386816601551 and error 0.025942553595160334\n",
      "iteration 350/500 of epoch 0/100, loss 0.7220346395564323 and error 0.023114045261372413\n",
      "iteration 400/500 of epoch 0/100, loss 0.7099657297867435 and error 0.020868673990443624\n",
      "iteration 450/500 of epoch 0/100, loss 0.7000252603808442 and error 0.019038580346164156\n",
      "iteration 0/500 of epoch 1/100, loss 0.6916825588516993 and error 0.0175150906931161\n",
      "iteration 50/500 of epoch 1/100, loss 0.6847051560087549 and error 0.016248639530460733\n",
      "iteration 100/500 of epoch 1/100, loss 0.6786671626535087 and error 0.015157428080134282\n",
      "iteration 150/500 of epoch 1/100, loss 0.6733883082789942 and error 0.014206173289979087\n",
      "iteration 200/500 of epoch 1/100, loss 0.6687325712887269 and error 0.013368608897655202\n",
      "iteration 250/500 of epoch 1/100, loss 0.6645952089241062 and error 0.012624761405800926\n",
      "iteration 300/500 of epoch 1/100, loss 0.6608941336072471 and error 0.011959158950095966\n",
      "iteration 350/500 of epoch 1/100, loss 0.6575640179419049 and error 0.011359620676801007\n",
      "iteration 400/500 of epoch 1/100, loss 0.6545521660098292 and error 0.01081641830370086\n",
      "iteration 450/500 of epoch 1/100, loss 0.6518155592265551 and error 0.010321682708008205\n",
      "iteration 0/500 of epoch 2/100, loss 0.6493187021203919 and error 0.009868975701452912\n",
      "iteration 50/500 of epoch 2/100, loss 0.6470758841529529 and error 0.009460970422428617\n",
      "iteration 100/500 of epoch 2/100, loss 0.6450114816535136 and error 0.009084029392823939\n",
      "iteration 150/500 of epoch 2/100, loss 0.6431055952952913 and error 0.008734619885858812\n",
      "iteration 200/500 of epoch 2/100, loss 0.6413412012594006 and error 0.008409737626820506\n",
      "iteration 250/500 of epoch 2/100, loss 0.6397036436912458 and error 0.008106810310900433\n",
      "iteration 300/500 of epoch 2/100, loss 0.6381802320675956 and error 0.007823621645808066\n",
      "iteration 350/500 of epoch 2/100, loss 0.6367599188344475 and error 0.007558250981944124\n",
      "iteration 400/500 of epoch 2/100, loss 0.6354330391087675 and error 0.007309024908935805\n",
      "iteration 450/500 of epoch 2/100, loss 0.634191098833184 and error 0.00707447813036739\n",
      "iteration 0/500 of epoch 3/100, loss 0.6330266010984312 and error 0.006853321598830178\n",
      "iteration 50/500 of epoch 3/100, loss 0.6319541286176784 and error 0.0066484827949708945\n",
      "iteration 100/500 of epoch 3/100, loss 0.6309440816584942 and error 0.006454457490284824\n",
      "iteration 150/500 of epoch 3/100, loss 0.6299914809047631 and error 0.0062703960158776335\n",
      "iteration 200/500 of epoch 3/100, loss 0.6290918585096398 and error 0.006095537401752787\n",
      "iteration 250/500 of epoch 3/100, loss 0.6282411937262716 and error 0.005929197910130464\n",
      "iteration 300/500 of epoch 3/100, loss 0.627435858098621 and error 0.0057707613219835354\n",
      "iteration 350/500 of epoch 3/100, loss 0.6266725685832847 and error 0.005619670669886503\n",
      "iteration 400/500 of epoch 3/100, loss 0.6259483472849351 and error 0.005475421170332491\n",
      "iteration 450/500 of epoch 3/100, loss 0.6252604867338544 and error 0.0053375541558197724\n",
      "iteration 0/500 of epoch 4/100, loss 0.6246065198292107 and error 0.00520565184425535\n",
      "iteration 50/500 of epoch 4/100, loss 0.6239963467030761 and error 0.005081807567367652\n",
      "iteration 100/500 of epoch 4/100, loss 0.6234146187030263 and error 0.004962996538857082\n",
      "iteration 150/500 of epoch 4/100, loss 0.6228595527329912 and error 0.004848916833016104\n",
      "iteration 200/500 of epoch 4/100, loss 0.6223295084470852 and error 0.0047392906125296565\n",
      "iteration 250/500 of epoch 4/100, loss 0.6218229742772104 and error 0.004633861738584707\n",
      "iteration 300/500 of epoch 4/100, loss 0.6213385550764514 and error 0.004532393663181205\n",
      "iteration 350/500 of epoch 4/100, loss 0.620874961163214 and error 0.0044346675651618424\n",
      "iteration 400/500 of epoch 4/100, loss 0.6204309985833448 and error 0.004340480697401497\n",
      "iteration 450/500 of epoch 4/100, loss 0.6200055604343706 and error 0.004249644917510527\n",
      "iteration 0/500 of epoch 5/100, loss 0.6195976191185018 and error 0.004161985378497088\n",
      "iteration 50/500 of epoch 5/100, loss 0.6192138926868032 and error 0.004079004181152254\n",
      "iteration 100/500 of epoch 5/100, loss 0.6188452150917321 and error 0.003998774273603681\n",
      "iteration 150/500 of epoch 5/100, loss 0.6184908049537001 and error 0.003921162208070589\n",
      "iteration 200/500 of epoch 5/100, loss 0.6181499322929223 and error 0.003846043139166236\n",
      "iteration 250/500 of epoch 5/100, loss 0.6178219144086575 and error 0.0037733001329397033\n",
      "iteration 300/500 of epoch 5/100, loss 0.6175061121486051 and error 0.003702823542084633\n",
      "iteration 350/500 of epoch 5/100, loss 0.6172019265259141 and error 0.0036345104399817035\n",
      "iteration 400/500 of epoch 5/100, loss 0.6169087956464946 and error 0.0035682641071628075\n",
      "iteration 450/500 of epoch 5/100, loss 0.6166261919138507 and error 0.003503993564576849\n",
      "iteration 0/500 of epoch 6/100, loss 0.6163536194825567 and error 0.0034416131487202115\n",
      "iteration 50/500 of epoch 6/100, loss 0.6160957821287552 and error 0.0033822365805671727\n",
      "iteration 100/500 of epoch 6/100, loss 0.6158467171958232 and error 0.0033245259949604145\n",
      "iteration 150/500 of epoch 6/100, loss 0.6156060345895514 and error 0.003268413598238423\n",
      "iteration 200/500 of epoch 6/100, loss 0.615373366030236 and error 0.0032138352549506465\n",
      "iteration 250/500 of epoch 6/100, loss 0.6151483635714305 and error 0.003160730241448236\n",
      "iteration 300/500 of epoch 6/100, loss 0.6149306982373604 and error 0.003109041019301944\n",
      "iteration 350/500 of epoch 6/100, loss 0.6147200587680458 and error 0.0030587130266997798\n",
      "iteration 400/500 of epoch 6/100, loss 0.6145161504623244 and error 0.0030096944861720673\n",
      "iteration 450/500 of epoch 6/100, loss 0.6143186941099733 and error 0.002961936227163737\n",
      "iteration 0/500 of epoch 7/100, loss 0.6141274250050209 and error 0.0029153915221257743\n",
      "iteration 50/500 of epoch 7/100, loss 0.6139457426394541 and error 0.002870912442726342\n",
      "iteration 100/500 of epoch 7/100, loss 0.6137695367901743 and error 0.002827516348230107\n",
      "iteration 150/500 of epoch 7/100, loss 0.6135985940464858 and error 0.002785165288459063\n",
      "iteration 200/500 of epoch 7/100, loss 0.6134327113879056 and error 0.0027438230702941267\n",
      "iteration 250/500 of epoch 7/100, loss 0.6132716955723259 and error 0.0027034551560587822\n",
      "iteration 300/500 of epoch 7/100, loss 0.6131153625666131 and error 0.002664028568941638\n",
      "iteration 350/500 of epoch 7/100, loss 0.6129635370162452 and error 0.002625511804892531\n",
      "iteration 400/500 of epoch 7/100, loss 0.6128160517509127 and error 0.002587874750478036\n",
      "iteration 450/500 of epoch 7/100, loss 0.6126727473232665 and error 0.002551088606228716\n",
      "iteration 0/500 of epoch 8/100, loss 0.612533471578255 and error 0.0025151258150518624\n",
      "iteration 50/500 of epoch 8/100, loss 0.6124007503149141 and error 0.00248065578883064\n",
      "iteration 100/500 of epoch 8/100, loss 0.6122716279898379 and error 0.0024469273706561883\n",
      "iteration 150/500 of epoch 8/100, loss 0.6121459796440819 and error 0.0024139177417420634\n",
      "iteration 200/500 of epoch 8/100, loss 0.612023685709243 and error 0.0023816050070294015\n",
      "iteration 250/500 of epoch 8/100, loss 0.6119046317272576 and error 0.002349968148525753\n",
      "iteration 300/500 of epoch 8/100, loss 0.6117887080873131 and error 0.0023189869814733088\n",
      "iteration 350/500 of epoch 8/100, loss 0.6116758097786719 and error 0.0022886421131470447\n",
      "iteration 400/500 of epoch 8/100, loss 0.6115658361583025 and error 0.0022589149040992433\n",
      "iteration 450/500 of epoch 8/100, loss 0.6114586907322933 and error 0.002229787431681349\n",
      "iteration 0/500 of epoch 9/100, loss 0.6113542809501096 and error 0.0022012424556873715\n",
      "iteration 50/500 of epoch 9/100, loss 0.6112545281387792 and error 0.0021738176137437305\n",
      "iteration 100/500 of epoch 9/100, loss 0.6111572370749386 and error 0.0021469212246698075\n",
      "iteration 150/500 of epoch 9/100, loss 0.6110623306403771 and error 0.002120538797098739\n",
      "iteration 200/500 of epoch 9/100, loss 0.610969734704084 and error 0.0020946563603884503\n",
      "iteration 250/500 of epoch 9/100, loss 0.6108793779832827 and error 0.0020692604413299525\n",
      "iteration 300/500 of epoch 9/100, loss 0.6107911919120375 and error 0.002044338042107378\n",
      "iteration 350/500 of epoch 9/100, loss 0.6107051105169693 and error 0.002019876619431221\n",
      "iteration 400/500 of epoch 9/100, loss 0.6106210702996282 and error 0.0019958640647720484\n",
      "iteration 450/500 of epoch 9/100, loss 0.6105390101251273 and error 0.001972288685626847\n",
      "iteration 0/500 of epoch 10/100, loss 0.6104588711166352 and error 0.0019491391877551104\n",
      "iteration 50/500 of epoch 10/100, loss 0.6103821443093344 and error 0.0019268554163083807\n",
      "iteration 100/500 of epoch 10/100, loss 0.6103071566406293 and error 0.001904960245880507\n",
      "iteration 150/500 of epoch 10/100, loss 0.6102338584728905 and error 0.0018834440694764087\n",
      "iteration 200/500 of epoch 10/100, loss 0.6101622019129255 and error 0.0018622975903312324\n",
      "iteration 250/500 of epoch 10/100, loss 0.6100921407385562 and error 0.0018415118094796746\n",
      "iteration 300/500 of epoch 10/100, loss 0.6100236303288021 and error 0.0018210780139233233\n",
      "iteration 350/500 of epoch 10/100, loss 0.6099566275974854 and error 0.0018009877653623666\n",
      "iteration 400/500 of epoch 10/100, loss 0.6098910909300436 and error 0.001781232889460233\n",
      "iteration 450/500 of epoch 10/100, loss 0.6098269801233948 and error 0.0017618054656117236\n",
      "iteration 0/500 of epoch 11/100, loss 0.6097642563286654 and error 0.0017426978171869536\n",
      "iteration 50/500 of epoch 11/100, loss 0.6097040966241135 and error 0.001724275444357316\n",
      "iteration 100/500 of epoch 11/100, loss 0.609645198754835 and error 0.001706146239891623\n",
      "iteration 150/500 of epoch 11/100, loss 0.6095875296491311 and error 0.0016883036106232138\n",
      "iteration 200/500 of epoch 11/100, loss 0.6095310572984882 and error 0.0016707411566448793\n",
      "iteration 250/500 of epoch 11/100, loss 0.6094757507167364 and error 0.0016534526643020482\n",
      "iteration 300/500 of epoch 11/100, loss 0.6094215799010344 and error 0.0016364320994904193\n",
      "iteration 350/500 of epoch 11/100, loss 0.6093685157945956 and error 0.0016196736012424997\n",
      "iteration 400/500 of epoch 11/100, loss 0.6093165302510611 and error 0.0016031714755885302\n",
      "iteration 450/500 of epoch 11/100, loss 0.6092655960004404 and error 0.001586920189678038\n",
      "iteration 0/500 of epoch 12/100, loss 0.6092156866165365 and error 0.0015709143661491076\n",
      "iteration 50/500 of epoch 12/100, loss 0.6091677451531288 and error 0.001555461806881387\n",
      "iteration 100/500 of epoch 12/100, loss 0.6091207399877284 and error 0.0015402351826583952\n",
      "iteration 150/500 of epoch 12/100, loss 0.6090746484391565 and error 0.0015252298379441484\n",
      "iteration 200/500 of epoch 12/100, loss 0.6090294484977109 and error 0.001510441242082858\n",
      "iteration 250/500 of epoch 12/100, loss 0.608985118801453 and error 0.0014958649851667559\n",
      "iteration 300/500 of epoch 12/100, loss 0.6089416386134773 and error 0.00148149677406734\n",
      "iteration 350/500 of epoch 12/100, loss 0.608898987800101 and error 0.0014673324286224125\n",
      "iteration 400/500 of epoch 12/100, loss 0.6088571468099494 and error 0.0014533678779718915\n",
      "iteration 450/500 of epoch 12/100, loss 0.6088160966538826 and error 0.0014395991570354935\n",
      "iteration 0/500 of epoch 13/100, loss 0.6087758188857261 and error 0.001426022403125946\n",
      "iteration 50/500 of epoch 13/100, loss 0.608737078841765 and error 0.001412899833683209\n",
      "iteration 100/500 of epoch 13/100, loss 0.6086990469868104 and error 0.001399954560704001\n",
      "iteration 150/500 of epoch 13/100, loss 0.6086617073774597 and error 0.00138718321730556\n",
      "iteration 200/500 of epoch 13/100, loss 0.608625044507403 and error 0.001374582519798257\n",
      "iteration 250/500 of epoch 13/100, loss 0.608589043293152 and error 0.0013621492651548087\n",
      "iteration 300/500 of epoch 13/100, loss 0.6085536890603138 and error 0.0013498803285712695\n",
      "iteration 350/500 of epoch 13/100, loss 0.6085189675303843 and error 0.0013377726611158397\n",
      "iteration 400/500 of epoch 13/100, loss 0.6084848648080322 and error 0.0013258232874619215\n",
      "iteration 450/500 of epoch 13/100, loss 0.6084513673688678 and error 0.001314029303701775\n",
      "iteration 0/500 of epoch 14/100, loss 0.6084184620476573 and error 0.0013023878752375247\n",
      "iteration 50/500 of epoch 14/100, loss 0.608386777007085 and error 0.001291124641699289\n",
      "iteration 100/500 of epoch 14/100, loss 0.6083556365736715 and error 0.0012800027197920599\n",
      "iteration 150/500 of epoch 14/100, loss 0.6083250292990535 and error 0.0012690196243609123\n",
      "iteration 200/500 of epoch 14/100, loss 0.6082949440268844 and error 0.0012581729271203647\n",
      "iteration 250/500 of epoch 14/100, loss 0.6082653698839822 and error 0.0012474602550541305\n",
      "iteration 300/500 of epoch 14/100, loss 0.6082362962717804 and error 0.0012368792888683958\n",
      "iteration 350/500 of epoch 14/100, loss 0.6082077128580909 and error 0.001226427761496544\n",
      "iteration 400/500 of epoch 14/100, loss 0.6081796095691417 and error 0.0012161034566533458\n",
      "iteration 450/500 of epoch 14/100, loss 0.6081519765818991 and error 0.0012059042074366853\n",
      "iteration 0/500 of epoch 15/100, loss 0.6081248043166546 and error 0.0011958278949750257\n",
      "iteration 50/500 of epoch 15/100, loss 0.608098613526769 and error 0.0011860704046997208\n",
      "iteration 100/500 of epoch 15/100, loss 0.6080728476579099 and error 0.0011764270841007737\n",
      "iteration 150/500 of epoch 15/100, loss 0.6080474983351536 and error 0.0011668960660918485\n",
      "iteration 200/500 of epoch 15/100, loss 0.6080225573831458 and error 0.0011574755233312807\n",
      "iteration 250/500 of epoch 15/100, loss 0.6079980168204577 and error 0.0011481636671823066\n",
      "iteration 300/500 of epoch 15/100, loss 0.6079738688541234 and error 0.0011389587467055269\n",
      "iteration 350/500 of epoch 15/100, loss 0.6079501058743599 and error 0.001129859047682513\n",
      "iteration 400/500 of epoch 15/100, loss 0.6079267204494571 and error 0.0011208628916694126\n",
      "iteration 450/500 of epoch 15/100, loss 0.6079037053208333 and error 0.001111968635079444\n",
      "iteration 0/500 of epoch 16/100, loss 0.6078810533982507 and error 0.0011031746682933412\n",
      "iteration 50/500 of epoch 16/100, loss 0.6078592002548007 and error 0.0010946523789048107\n",
      "iteration 100/500 of epoch 16/100, loss 0.6078376829096361 and error 0.0010862234394505483\n",
      "iteration 150/500 of epoch 16/100, loss 0.6078164951349644 and error 0.0010778864252450903\n",
      "iteration 200/500 of epoch 16/100, loss 0.6077956308421304 and error 0.001069639939919203\n",
      "iteration 250/500 of epoch 16/100, loss 0.6077750840779341 and error 0.001061482614728219\n",
      "iteration 300/500 of epoch 16/100, loss 0.6077548490210599 and error 0.0010534131078803724\n",
      "iteration 350/500 of epoch 16/100, loss 0.6077349199786116 and error 0.001045430103884492\n",
      "iteration 400/500 of epoch 16/100, loss 0.6077152913827566 and error 0.0010375323129164118\n",
      "iteration 450/500 of epoch 16/100, loss 0.6076959577874737 and error 0.001029718470203426\n",
      "iteration 0/500 of epoch 17/100, loss 0.6076769138653912 and error 0.0010219873354262617\n",
      "iteration 50/500 of epoch 17/100, loss 0.6076585268667721 and error 0.001014489907767455\n",
      "iteration 100/500 of epoch 17/100, loss 0.6076404082632632 and error 0.0010070696209006048\n",
      "iteration 150/500 of epoch 17/100, loss 0.6076225533558481 and error 0.0009997253726498738\n",
      "iteration 200/500 of epoch 17/100, loss 0.6076049575442544 and error 0.000992456081357997\n",
      "iteration 250/500 of epoch 17/100, loss 0.6075876163244962 and error 0.000985260685416667\n",
      "iteration 300/500 of epoch 17/100, loss 0.6075705252864886 and error 0.0009781381428096234\n",
      "iteration 350/500 of epoch 17/100, loss 0.6075536801117346 and error 0.0009710874306680885\n",
      "iteration 400/500 of epoch 17/100, loss 0.6075370765710701 and error 0.0009641075448381368\n",
      "iteration 450/500 of epoch 17/100, loss 0.6075207105224828 and error 0.00095719749945964\n",
      "iteration 0/500 of epoch 18/100, loss 0.6075045779089848 and error 0.0009503563265564197\n",
      "iteration 50/500 of epoch 18/100, loss 0.6074889906183053 and error 0.000943717892517946\n",
      "iteration 100/500 of epoch 18/100, loss 0.6074736200361694 and error 0.0009371438117441922\n",
      "iteration 150/500 of epoch 18/100, loss 0.6074584625706403 and error 0.0009306332210088955\n",
      "iteration 200/500 of epoch 18/100, loss 0.6074435147009736 and error 0.0009241852721796348\n",
      "iteration 250/500 of epoch 18/100, loss 0.6074287729759529 and error 0.0009177991318931694\n",
      "iteration 300/500 of epoch 18/100, loss 0.6074142340122666 and error 0.0009114739812390378\n",
      "iteration 350/500 of epoch 18/100, loss 0.607399894492928 and error 0.0009052090154511679\n",
      "iteration 400/500 of epoch 18/100, loss 0.6073857511657428 and error 0.0008990034436072975\n",
      "iteration 450/500 of epoch 18/100, loss 0.6073718008418122 and error 0.0008928564883359325\n",
      "iteration 0/500 of epoch 19/100, loss 0.6073580403940787 and error 0.0008867673855306546\n",
      "iteration 50/500 of epoch 19/100, loss 0.607344736435508 and error 0.0008808554789764958\n",
      "iteration 100/500 of epoch 19/100, loss 0.6073316090568319 and error 0.0008749977171828481\n",
      "iteration 150/500 of epoch 19/100, loss 0.6073186554800618 and error 0.0008691934165454396\n",
      "iteration 200/500 of epoch 19/100, loss 0.6073058729792773 and error 0.000863441904714344\n",
      "iteration 250/500 of epoch 19/100, loss 0.6072932588794765 and error 0.0008577425203658994\n",
      "iteration 300/500 of epoch 19/100, loss 0.6072808105554519 and error 0.0008520946129800769\n",
      "iteration 350/500 of epoch 19/100, loss 0.6072685254307016 and error 0.0008464975426231881\n",
      "iteration 400/500 of epoch 19/100, loss 0.607256400976358 and error 0.0008409506797357476\n",
      "iteration 450/500 of epoch 19/100, loss 0.607244434710155 and error 0.0008354534049253841\n",
      "iteration 0/500 of epoch 20/100, loss 0.6072326241954118 and error 0.000830005108764629\n",
      "iteration 50/500 of epoch 20/100, loss 0.6072211987102982 and error 0.0008247127278987866\n",
      "iteration 100/500 of epoch 20/100, loss 0.6072099183002141 and error 0.0008194662545239875\n",
      "iteration 150/500 of epoch 20/100, loss 0.6071987807938402 and error 0.0008142651418335872\n",
      "iteration 200/500 of epoch 20/100, loss 0.6071877840584318 and error 0.0008091088515155312\n",
      "iteration 250/500 of epoch 20/100, loss 0.6071769259990133 and error 0.0008039968535897905\n",
      "iteration 300/500 of epoch 20/100, loss 0.6071662045575902 and error 0.0007989286262494713\n",
      "iteration 350/500 of epoch 20/100, loss 0.6071556177123812 and error 0.0007939036557054876\n",
      "iteration 400/500 of epoch 20/100, loss 0.6071451634770685 and error 0.0007889214360347856\n",
      "iteration 450/500 of epoch 20/100, loss 0.6071348399000632 and error 0.0007839814690318986\n",
      "iteration 0/500 of epoch 21/100, loss 0.6071246450637949 and error 0.0007790832640638829\n",
      "iteration 50/500 of epoch 21/100, loss 0.6071147772246966 and error 0.0007743230820821143\n",
      "iteration 100/500 of epoch 21/100, loss 0.6071050294750862 and error 0.0007696020961248162\n",
      "iteration 150/500 of epoch 21/100, loss 0.6070954001014509 and error 0.0007649198648080369\n",
      "iteration 200/500 of epoch 21/100, loss 0.6070858874191949 and error 0.0007602759532309683\n",
      "iteration 250/500 of epoch 21/100, loss 0.6070764897720656 and error 0.0007556699328584997\n",
      "iteration 300/500 of epoch 21/100, loss 0.6070672055315907 and error 0.0007511013814063557\n",
      "iteration 350/500 of epoch 21/100, loss 0.6070580330965343 and error 0.0007465698827286586\n",
      "iteration 400/500 of epoch 21/100, loss 0.6070489708923597 and error 0.0007420750267078873\n",
      "iteration 450/500 of epoch 21/100, loss 0.6070400173707065 and error 0.000737616409147194\n",
      "iteration 0/500 of epoch 22/100, loss 0.6070311710088804 and error 0.0007331936316649706\n",
      "iteration 50/500 of epoch 22/100, loss 0.607022604107634 and error 0.0007288937094276225\n",
      "iteration 100/500 of epoch 22/100, loss 0.607014137297639 and error 0.000724627466390651\n",
      "iteration 150/500 of epoch 22/100, loss 0.6070057692147708 and error 0.000720394543294303\n",
      "iteration 200/500 of epoch 22/100, loss 0.6069974985168136 and error 0.0007161945858771828\n",
      "iteration 250/500 of epoch 22/100, loss 0.6069893238830464 and error 0.0007120272447904226\n",
      "iteration 300/500 of epoch 22/100, loss 0.6069812440138416 and error 0.00070789217551366\n",
      "iteration 350/500 of epoch 22/100, loss 0.6069732576302673 and error 0.0007037890382726704\n",
      "iteration 400/500 of epoch 22/100, loss 0.6069653634737029 and error 0.0006997174979586886\n",
      "iteration 450/500 of epoch 22/100, loss 0.6069575603054601 and error 0.0006956772240493395\n",
      "iteration 0/500 of epoch 23/100, loss 0.6069498469064128 and error 0.0006916678905311755\n",
      "iteration 50/500 of epoch 23/100, loss 0.6069423737214874 and error 0.0006877684574135255\n",
      "iteration 100/500 of epoch 23/100, loss 0.6069349844864073 and error 0.00068389813187775\n",
      "iteration 150/500 of epoch 23/100, loss 0.6069276781064509 and error 0.0006800566192731256\n",
      "iteration 200/500 of epoch 23/100, loss 0.606920453503659 and error 0.0006762436288385161\n",
      "iteration 250/500 of epoch 23/100, loss 0.6069133096165338 and error 0.0006724588736389956\n",
      "iteration 300/500 of epoch 23/100, loss 0.6069062453997467 and error 0.0006687020705036877\n",
      "iteration 350/500 of epoch 23/100, loss 0.6068992598238478 and error 0.0006649729399647842\n",
      "iteration 400/500 of epoch 23/100, loss 0.6068923518749842 and error 0.0006612712061977532\n",
      "iteration 450/500 of epoch 23/100, loss 0.6068855205546253 and error 0.0006575965969626894\n",
      "iteration 0/500 of epoch 24/100, loss 0.6068787648792875 and error 0.0006539488435467648\n",
      "iteration 50/500 of epoch 24/100, loss 0.6068722167818488 and error 0.0006503998497014329\n",
      "iteration 100/500 of epoch 24/100, loss 0.6068657395051302 and error 0.0006468761468887708\n",
      "iteration 150/500 of epoch 24/100, loss 0.6068593321641097 and error 0.0006433774917503693\n",
      "iteration 200/500 of epoch 24/100, loss 0.6068529938867064 and error 0.000639903643980613\n",
      "iteration 250/500 of epoch 24/100, loss 0.6068467238135585 and error 0.0006364543662793766\n",
      "iteration 300/500 of epoch 24/100, loss 0.6068405210978085 and error 0.0006330294243055883\n",
      "iteration 350/500 of epoch 24/100, loss 0.6068343849048903 and error 0.0006296285866316378\n",
      "iteration 400/500 of epoch 24/100, loss 0.6068283144123192 and error 0.0006262516246986206\n",
      "iteration 450/500 of epoch 24/100, loss 0.6068223088094895 and error 0.000622898312772405\n",
      "iteration 0/500 of epoch 25/100, loss 0.606816367297474 and error 0.000619568427900476\n",
      "iteration 50/500 of epoch 25/100, loss 0.6068106060437926 and error 0.000616327661469307\n",
      "iteration 100/500 of epoch 25/100, loss 0.606804904855865 and error 0.0006131089780592929\n",
      "iteration 150/500 of epoch 25/100, loss 0.6067992630133184 and error 0.0006099121753740698\n",
      "iteration 200/500 of epoch 25/100, loss 0.6067936798058553 and error 0.0006067370535322935\n",
      "iteration 250/500 of epoch 25/100, loss 0.6067881545330885 and error 0.0006035834150320016\n",
      "iteration 300/500 of epoch 25/100, loss 0.6067826865043806 and error 0.0006004510647155971\n",
      "iteration 350/500 of epoch 25/100, loss 0.6067772750386825 and error 0.0005973398097354553\n",
      "iteration 400/500 of epoch 25/100, loss 0.606771919464383 and error 0.0005942494595201103\n",
      "iteration 450/500 of epoch 25/100, loss 0.6067666191191528 and error 0.000591179825741051\n",
      "iteration 0/500 of epoch 26/100, loss 0.6067613733497967 and error 0.0005881307222800748\n",
      "iteration 50/500 of epoch 26/100, loss 0.6067562848296777 and error 0.0005851623456937303\n",
      "iteration 100/500 of epoch 26/100, loss 0.6067512475072981 and error 0.000582213338579767\n",
      "iteration 150/500 of epoch 26/100, loss 0.6067462607926076 and error 0.000579283531768955\n",
      "iteration 200/500 of epoch 26/100, loss 0.6067413241034563 and error 0.0005763727580165496\n",
      "iteration 250/500 of epoch 26/100, loss 0.6067364368654762 and error 0.0005734808519752219\n",
      "iteration 300/500 of epoch 26/100, loss 0.6067315985119578 and error 0.0005706076501684328\n",
      "iteration 350/500 of epoch 26/100, loss 0.6067268084837327 and error 0.0005677529909642533\n",
      "iteration 400/500 of epoch 26/100, loss 0.6067220662290546 and error 0.0005649167145496123\n",
      "iteration 450/500 of epoch 26/100, loss 0.6067173712034858 and error 0.0005620986629049923\n",
      "iteration 0/500 of epoch 27/100, loss 0.6067127228697862 and error 0.0005592986797795138\n",
      "iteration 50/500 of epoch 27/100, loss 0.6067082122967237 and error 0.0005565720806381622\n",
      "iteration 100/500 of epoch 27/100, loss 0.6067037455647741 and error 0.0005538625422405886\n",
      "iteration 150/500 of epoch 27/100, loss 0.6066993221878015 and error 0.0005511699223351749\n",
      "iteration 200/500 of epoch 27/100, loss 0.6066949416859164 and error 0.0005484940802143151\n",
      "iteration 250/500 of epoch 27/100, loss 0.6066906035853783 and error 0.0005458348766937031\n",
      "iteration 300/500 of epoch 27/100, loss 0.606686307418507 and error 0.0005431921740919096\n",
      "iteration 350/500 of epoch 27/100, loss 0.6066820527235924 and error 0.000540565836210311\n",
      "iteration 400/500 of epoch 27/100, loss 0.6066778390448039 and error 0.0005379557283133141\n",
      "iteration 450/500 of epoch 27/100, loss 0.6066736659321063 and error 0.0005353617171089225\n",
      "iteration 0/500 of epoch 28/100, loss 0.6066695329411722 and error 0.0005327836707295816\n",
      "iteration 50/500 of epoch 28/100, loss 0.6066655211172202 and error 0.0005302725514138834\n",
      "iteration 100/500 of epoch 28/100, loss 0.6066615469970402 and error 0.0005277765178758822\n",
      "iteration 150/500 of epoch 28/100, loss 0.6066576101779333 and error 0.0005252954498819669\n",
      "iteration 200/500 of epoch 28/100, loss 0.6066537102621676 and error 0.0005228292284451267\n",
      "iteration 250/500 of epoch 28/100, loss 0.6066498468569043 and error 0.0005203777358089713\n",
      "iteration 300/500 of epoch 28/100, loss 0.6066460195741314 and error 0.0005179408554319608\n",
      "iteration 350/500 of epoch 28/100, loss 0.6066422280305935 and error 0.0005155184719718957\n",
      "iteration 400/500 of epoch 28/100, loss 0.606638471847724 and error 0.0005131104712706287\n",
      "iteration 450/500 of epoch 28/100, loss 0.6066347506515783 and error 0.0005107167403390168\n",
      "iteration 0/500 of epoch 29/100, loss 0.6066310640727705 and error 0.0005083371673421043\n",
      "iteration 50/500 of epoch 29/100, loss 0.6066274844629197 and error 0.0005060188176399212\n",
      "iteration 100/500 of epoch 29/100, loss 0.606623937409726 and error 0.0005037138557290444\n",
      "iteration 150/500 of epoch 29/100, loss 0.6066204225779123 and error 0.0005014221794980137\n",
      "iteration 200/500 of epoch 29/100, loss 0.6066169396361767 and error 0.0004991436878477888\n",
      "iteration 250/500 of epoch 29/100, loss 0.6066134882571373 and error 0.0004968782806793174\n",
      "iteration 300/500 of epoch 29/100, loss 0.6066100681172768 and error 0.0004946258588812702\n",
      "iteration 350/500 of epoch 29/100, loss 0.6066066788968919 and error 0.0004923863243179854\n",
      "iteration 400/500 of epoch 29/100, loss 0.6066033202800414 and error 0.0004901595798175653\n",
      "iteration 450/500 of epoch 29/100, loss 0.6065999919544931 and error 0.0004879455291601399\n",
      "iteration 0/500 of epoch 30/100, loss 0.6065966936116745 and error 0.0004857440770663371\n",
      "iteration 50/500 of epoch 30/100, loss 0.6065934900339174 and error 0.00048359878843429085\n",
      "iteration 100/500 of epoch 30/100, loss 0.6065903146749101 and error 0.00048146542086796774\n",
      "iteration 150/500 of epoch 30/100, loss 0.6065871672541869 and error 0.0004793438872580294\n",
      "iteration 200/500 of epoch 30/100, loss 0.60658404749448 and error 0.00047723410132187417\n",
      "iteration 250/500 of epoch 30/100, loss 0.6065809551216781 and error 0.0004751359775939246\n",
      "iteration 300/500 of epoch 30/100, loss 0.6065778898647849 and error 0.00047304943141603144\n",
      "iteration 350/500 of epoch 30/100, loss 0.6065748514558792 and error 0.0004709743789280247\n",
      "iteration 400/500 of epoch 30/100, loss 0.6065718396300729 and error 0.0004689107370583833\n",
      "iteration 450/500 of epoch 30/100, loss 0.6065688541254728 and error 0.0004668584235150537\n",
      "iteration 0/500 of epoch 31/100, loss 0.6065658946831403 and error 0.0004648173567763837\n",
      "iteration 50/500 of epoch 31/100, loss 0.6065630194710213 and error 0.00046282794718187044\n",
      "iteration 100/500 of epoch 31/100, loss 0.6065601688048823 and error 0.00046084918611753354\n",
      "iteration 150/500 of epoch 31/100, loss 0.6065573424490627 and error 0.000458880998960609\n",
      "iteration 200/500 of epoch 31/100, loss 0.6065545401704916 and error 0.0004569233117669204\n",
      "iteration 250/500 of epoch 31/100, loss 0.6065517617386539 and error 0.0004549760512632317\n",
      "iteration 300/500 of epoch 31/100, loss 0.6065490069255576 and error 0.00045303914483970194\n",
      "iteration 350/500 of epoch 31/100, loss 0.6065462755057024 and error 0.0004511125205424395\n",
      "iteration 400/500 of epoch 31/100, loss 0.6065435672560472 and error 0.00044919610706614204\n",
      "iteration 450/500 of epoch 31/100, loss 0.6065408819559827 and error 0.00044728983374686253\n",
      "iteration 0/500 of epoch 32/100, loss 0.6065382193872968 and error 0.00044539363055484535\n",
      "iteration 50/500 of epoch 32/100, loss 0.6065356319181275 and error 0.00044354505636715774\n",
      "iteration 100/500 of epoch 32/100, loss 0.6065330658718607 and error 0.0004417060221068386\n",
      "iteration 150/500 of epoch 32/100, loss 0.6065305210496583 and error 0.00043987646359673323\n",
      "iteration 200/500 of epoch 32/100, loss 0.6065279972547885 and error 0.0004380563172193629\n",
      "iteration 250/500 of epoch 32/100, loss 0.6065254942925979 and error 0.00043624551991087524\n",
      "iteration 300/500 of epoch 32/100, loss 0.6065230119704899 and error 0.0004344440091550599\n",
      "iteration 350/500 of epoch 32/100, loss 0.6065205500978941 and error 0.00043265172297744456\n",
      "iteration 400/500 of epoch 32/100, loss 0.6065181084862481 and error 0.00043086859993949646\n",
      "iteration 450/500 of epoch 32/100, loss 0.6065156869489675 and error 0.0004290945791328306\n",
      "iteration 0/500 of epoch 33/100, loss 0.6065132853014273 and error 0.0004273296001735787\n",
      "iteration 50/500 of epoch 33/100, loss 0.6065109508097005 and error 0.0004256086371111798\n",
      "iteration 100/500 of epoch 33/100, loss 0.6065086350744109 and error 0.00042389624445678776\n",
      "iteration 150/500 of epoch 33/100, loss 0.6065063379271312 and error 0.00042219236681316307\n",
      "iteration 200/500 of epoch 33/100, loss 0.6065040592011541 and error 0.00042049694924675685\n",
      "iteration 250/500 of epoch 33/100, loss 0.6065017987314718 and error 0.00041880993728288376\n",
      "iteration 300/500 of epoch 33/100, loss 0.606499556354758 and error 0.000417131276900974\n",
      "iteration 350/500 of epoch 33/100, loss 0.6064973319093468 and error 0.00041546091452987954\n",
      "iteration 400/500 of epoch 33/100, loss 0.6064951252352142 and error 0.0004137987970432191\n",
      "iteration 450/500 of epoch 33/100, loss 0.6064929361739584 and error 0.00041214487175481187\n",
      "iteration 0/500 of epoch 34/100, loss 0.60649076456878 and error 0.0004104990864141375\n",
      "iteration 50/500 of epoch 34/100, loss 0.6064886531836808 and error 0.00040889406567244495\n",
      "iteration 100/500 of epoch 34/100, loss 0.6064865582696711 and error 0.00040729676437610286\n",
      "iteration 150/500 of epoch 34/100, loss 0.6064844796835539 and error 0.00040570713454077343\n",
      "iteration 200/500 of epoch 34/100, loss 0.6064824172835444 and error 0.0004041251285679261\n",
      "iteration 250/500 of epoch 34/100, loss 0.606480370929254 and error 0.0004025506992409734\n",
      "iteration 300/500 of epoch 34/100, loss 0.6064783404816758 and error 0.00040098379972147453\n",
      "iteration 350/500 of epoch 34/100, loss 0.6064763258031662 and error 0.0003994243835453616\n",
      "iteration 400/500 of epoch 34/100, loss 0.6064743267574328 and error 0.00039787240461924123\n",
      "iteration 450/500 of epoch 34/100, loss 0.6064723432095175 and error 0.00039632781721670986\n",
      "iteration 0/500 of epoch 35/100, loss 0.6064703750257817 and error 0.0003947905759747243\n",
      "iteration 50/500 of epoch 35/100, loss 0.6064684609859873 and error 0.0003932911647481944\n",
      "iteration 100/500 of epoch 35/100, loss 0.6064665614514791 and error 0.00039179872351192063\n",
      "iteration 150/500 of epoch 35/100, loss 0.6064646763000857 and error 0.0003903132105667382\n",
      "iteration 200/500 of epoch 35/100, loss 0.6064628054108003 and error 0.00038883458453577293\n",
      "iteration 250/500 of epoch 35/100, loss 0.6064609486637674 and error 0.00038736280436132644\n",
      "iteration 300/500 of epoch 35/100, loss 0.6064591059402701 and error 0.0003858978293018418\n",
      "iteration 350/500 of epoch 35/100, loss 0.606457277122719 and error 0.0003844396189288529\n",
      "iteration 400/500 of epoch 35/100, loss 0.6064554620946386 and error 0.0003829881331240171\n",
      "iteration 450/500 of epoch 35/100, loss 0.6064536607406561 and error 0.0003815433320761459\n",
      "iteration 0/500 of epoch 36/100, loss 0.6064518729464892 and error 0.00038010517627828867\n",
      "iteration 50/500 of epoch 36/100, loss 0.606450133956143 and error 0.00037870219422859704\n",
      "iteration 100/500 of epoch 36/100, loss 0.6064484077743459 and error 0.0003773055199877662\n",
      "iteration 150/500 of epoch 36/100, loss 0.6064466942965208 and error 0.00037591511720686026\n",
      "iteration 200/500 of epoch 36/100, loss 0.6064449934190531 and error 0.00037453094980719143\n",
      "iteration 250/500 of epoch 36/100, loss 0.6064433050392829 and error 0.00037315298197780606\n",
      "iteration 300/500 of epoch 36/100, loss 0.6064416290554926 and error 0.0003717811781730276\n",
      "iteration 350/500 of epoch 36/100, loss 0.6064399653668993 and error 0.00037041550311001087\n",
      "iteration 400/500 of epoch 36/100, loss 0.6064383138736437 and error 0.0003690559217663273\n",
      "iteration 450/500 of epoch 36/100, loss 0.6064366744767796 and error 0.00036770239937756663\n",
      "iteration 0/500 of epoch 37/100, loss 0.6064350470782667 and error 0.0003663549014349955\n",
      "iteration 50/500 of epoch 37/100, loss 0.6064334637760548 and error 0.00036504016644146447\n",
      "iteration 100/500 of epoch 37/100, loss 0.6064318918131331 and error 0.00036373115240758395\n",
      "iteration 150/500 of epoch 37/100, loss 0.6064303310997025 and error 0.00036242782755592967\n",
      "iteration 200/500 of epoch 37/100, loss 0.6064287815467665 and error 0.00036113016033648045\n",
      "iteration 250/500 of epoch 37/100, loss 0.60642724306612 and error 0.0003598381194246108\n",
      "iteration 300/500 of epoch 37/100, loss 0.6064257155703426 and error 0.0003585516737190814\n",
      "iteration 350/500 of epoch 37/100, loss 0.6064241989727901 and error 0.000357270792340058\n",
      "iteration 400/500 of epoch 37/100, loss 0.6064226931875869 and error 0.00035599544462715157\n",
      "iteration 450/500 of epoch 37/100, loss 0.6064211981296193 and error 0.00035472560013747814\n",
      "iteration 0/500 of epoch 38/100, loss 0.6064197137145259 and error 0.00035346122864375254\n",
      "iteration 50/500 of epoch 38/100, loss 0.6064182692338903 and error 0.00035222742654405674\n",
      "iteration 100/500 of epoch 38/100, loss 0.6064168348163377 and error 0.0003509988238224002\n",
      "iteration 150/500 of epoch 38/100, loss 0.6064154103845318 and error 0.0003497753926214053\n",
      "iteration 200/500 of epoch 38/100, loss 0.6064139958618038 and error 0.00034855710527568654\n",
      "iteration 250/500 of epoch 38/100, loss 0.6064125911721466 and error 0.0003473439343102457\n",
      "iteration 300/500 of epoch 38/100, loss 0.6064111962402058 and error 0.00034613585243882587\n",
      "iteration 350/500 of epoch 38/100, loss 0.6064098109912772 and error 0.00034493283256230815\n",
      "iteration 400/500 of epoch 38/100, loss 0.6064084353512967 and error 0.0003437348477671032\n",
      "iteration 450/500 of epoch 38/100, loss 0.6064070692468377 and error 0.00034254187132358216\n",
      "iteration 0/500 of epoch 39/100, loss 0.6064057126051001 and error 0.0003413538766845003\n",
      "iteration 50/500 of epoch 39/100, loss 0.6064043922082828 and error 0.0003401944507766244\n",
      "iteration 100/500 of epoch 39/100, loss 0.6064030807627746 and error 0.00033903975941301453\n",
      "iteration 150/500 of epoch 39/100, loss 0.6064017782017836 and error 0.00033788977810902504\n",
      "iteration 200/500 of epoch 39/100, loss 0.6064004844590772 and error 0.00033674448254266456\n",
      "iteration 250/500 of epoch 39/100, loss 0.6063991994689738 and error 0.00033560384855325665\n",
      "iteration 300/500 of epoch 39/100, loss 0.6063979231663421 and error 0.0003344678521401058\n",
      "iteration 350/500 of epoch 39/100, loss 0.6063966554865913 and error 0.0003333364694611859\n",
      "iteration 400/500 of epoch 39/100, loss 0.6063953963656705 and error 0.00033220967683181865\n",
      "iteration 450/500 of epoch 39/100, loss 0.6063941457400587 and error 0.0003310874507234026\n",
      "iteration 0/500 of epoch 40/100, loss 0.606392903546765 and error 0.00032996976776211067\n",
      "iteration 50/500 of epoch 40/100, loss 0.6063916943189795 and error 0.00032887882466749326\n",
      "iteration 100/500 of epoch 40/100, loss 0.6063904930712691 and error 0.0003277922007891886\n",
      "iteration 150/500 of epoch 40/100, loss 0.6063892997457937 and error 0.00032670987455371206\n",
      "iteration 200/500 of epoch 40/100, loss 0.6063881142851816 and error 0.0003256318245257786\n",
      "iteration 250/500 of epoch 40/100, loss 0.6063869366325251 and error 0.0003245580294072176\n",
      "iteration 300/500 of epoch 40/100, loss 0.606385766731378 and error 0.0003234884680358534\n",
      "iteration 350/500 of epoch 40/100, loss 0.606384604525749 and error 0.0003224231193844535\n",
      "iteration 400/500 of epoch 40/100, loss 0.6063834499600983 and error 0.00032136196255963577\n",
      "iteration 450/500 of epoch 40/100, loss 0.6063823029793345 and error 0.0003203049768008196\n",
      "iteration 0/500 of epoch 41/100, loss 0.6063811635288094 and error 0.0003192521414791657\n",
      "iteration 50/500 of epoch 41/100, loss 0.6063800541216152 and error 0.00031822437061792847\n",
      "iteration 100/500 of epoch 41/100, loss 0.606378951843774 and error 0.0003172005469707557\n",
      "iteration 150/500 of epoch 41/100, loss 0.606377856645069 and error 0.0003161806514845616\n",
      "iteration 200/500 of epoch 41/100, loss 0.6063767684756778 and error 0.00031516466522401297\n",
      "iteration 250/500 of epoch 41/100, loss 0.6063756872861702 and error 0.00031415256937064443\n",
      "iteration 300/500 of epoch 41/100, loss 0.6063746130275021 and error 0.00031314434522195715\n",
      "iteration 350/500 of epoch 41/100, loss 0.6063735456510149 and error 0.0003121399741905144\n",
      "iteration 400/500 of epoch 41/100, loss 0.6063724851084301 and error 0.00031113943780308197\n",
      "iteration 450/500 of epoch 41/100, loss 0.6063714313518467 and error 0.0003101427176997278\n",
      "iteration 0/500 of epoch 42/100, loss 0.6063703843337386 and error 0.00030914979563298337\n",
      "iteration 50/500 of epoch 42/100, loss 0.6063693647488638 and error 0.0003081804000763888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eduardoravkin/Dropbox/My Mac (Eduardâ€™s MacBook Pro)/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iter_per_epoch):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=8'>9</a>\u001b[0m     \u001b[39m# compute loss and gradient\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=9'>10</a>\u001b[0m     cov_inv \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39mdot(C\u001b[39m.\u001b[39mT)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=10'>11</a>\u001b[0m     loss_val, grad_loss_val \u001b[39m=\u001b[39m fAndG_invbarrier(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=11'>12</a>\u001b[0m         B \u001b[39m=\u001b[39;49m cov_gl,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=12'>13</a>\u001b[0m         C \u001b[39m=\u001b[39;49m C,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=13'>14</a>\u001b[0m         X \u001b[39m=\u001b[39;49m X,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=14'>15</a>\u001b[0m         a \u001b[39m=\u001b[39;49m regul_lambda\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=16'>17</a>\u001b[0m     error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(grad_loss_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/experiments/variance_approx_1700_14mar22/variance_cov_approx.ipynb#ch0000004?line=18'>19</a>\u001b[0m     \u001b[39m# update C (- because we are maximizing)\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/My Mac (Eduardâ€™s MacBook Pro)/Desktop/ml_research/PhD/preconditioners/src/preconditioners/variance_cov_approx.py:88\u001b[0m, in \u001b[0;36mfAndG_invbarrier\u001b[0;34m(B, C, X, a)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/src/preconditioners/variance_cov_approx.py?line=85'>86</a>\u001b[0m invbarrier_val, invbarrier_grad \u001b[39m=\u001b[39m invbarrier(C)\n\u001b[1;32m     <a href='file:///Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/src/preconditioners/variance_cov_approx.py?line=86'>87</a>\u001b[0m functionValue \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mtrace(((((((((T_0)\u001b[39m.\u001b[39mdot(X))\u001b[39m.\u001b[39mdot(C))\u001b[39m.\u001b[39mdot(C\u001b[39m.\u001b[39mT))\u001b[39m.\u001b[39mdot(B))\u001b[39m.\u001b[39mdot(C))\u001b[39m.\u001b[39mdot(C\u001b[39m.\u001b[39mT))\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT))\u001b[39m.\u001b[39mdot(T_0)) \u001b[39m+\u001b[39m a \u001b[39m*\u001b[39m invbarrier_val)\n\u001b[0;32m---> <a href='file:///Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/src/preconditioners/variance_cov_approx.py?line=87'>88</a>\u001b[0m gradient \u001b[39m=\u001b[39m (((((\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m T_2) \u001b[39m-\u001b[39m (T_4 \u001b[39m+\u001b[39m T_3)) \u001b[39m+\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (((((((B)\u001b[39m.\u001b[39;49mdot(C))\u001b[39m.\u001b[39;49mdot(C\u001b[39m.\u001b[39;49mT))\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT))\u001b[39m.\u001b[39mdot(T_0))\u001b[39m.\u001b[39mdot(T_0))\u001b[39m.\u001b[39mdot(X))\u001b[39m.\u001b[39mdot(C))) \u001b[39m-\u001b[39m (T_3 \u001b[39m+\u001b[39m T_4)) \u001b[39m+\u001b[39m a \u001b[39m*\u001b[39m invbarrier_grad)\n\u001b[1;32m     <a href='file:///Users/eduardoravkin/Dropbox/My%20Mac%20%28Eduard%E2%80%99s%20MacBook%20Pro%29/Desktop/ml_research/PhD/preconditioners/src/preconditioners/variance_cov_approx.py?line=89'>90</a>\u001b[0m \u001b[39mreturn\u001b[39;00m functionValue, gradient\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run optimization\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch == 0:\n",
    "        lr = lr_start\n",
    "    else:\n",
    "        lr = lr * lr_decay\n",
    "\n",
    "    for i in range(iter_per_epoch):\n",
    "        # compute loss and gradient\n",
    "        cov_inv = C.dot(C.T)\n",
    "        loss_val, grad_loss_val = fAndG_invbarrier(\n",
    "            B = cov_gl,\n",
    "            C = C,\n",
    "            X = X,\n",
    "            a = regul_lambda\n",
    "        )\n",
    "        error = np.linalg.norm(grad_loss_val)\n",
    "\n",
    "        # update C (- because we are maximizing)\n",
    "        C = C - lr * grad_loss_val\n",
    "        # update regul_lambda\n",
    "        # regul_lambda = regul_lambda - lr*np.trace(grad_loss_val.dot(grad_loss_val.T))\n",
    "        # check if we are done\n",
    "        if i % 50 == 0:\n",
    "            print(f\"iteration {i}/{iter_per_epoch} of epoch {epoch}/{n_epochs}, loss {loss_val} and error {error}\")\n",
    "        if error < tol:\n",
    "            break\n",
    "    if error < tol:\n",
    "        break\n",
    "\n",
    "dtstamp = str(dt.now()).replace(' ', '_')\n",
    "with open(f'results_{dtstamp}.json', 'w') as f:\n",
    "    json.dump({'C' : C.tolist(), 'loss' : float(loss_val), 'error' : float(error), 'parans' : params}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.00000000e-01, 2.50000000e-01, ...,\n",
       "        3.92727477e-90, 1.96363739e-90, 9.81818693e-91],\n",
       "       [5.00000000e-01, 1.00000000e+00, 5.00000000e-01, ...,\n",
       "        7.85454954e-90, 3.92727477e-90, 1.96363739e-90],\n",
       "       [2.50000000e-01, 5.00000000e-01, 1.00000000e+00, ...,\n",
       "        1.57090991e-89, 7.85454954e-90, 3.92727477e-90],\n",
       "       ...,\n",
       "       [3.92727477e-90, 7.85454954e-90, 1.57090991e-89, ...,\n",
       "        1.00000000e+00, 5.00000000e-01, 2.50000000e-01],\n",
       "       [1.96363739e-90, 3.92727477e-90, 7.85454954e-90, ...,\n",
       "        5.00000000e-01, 1.00000000e+00, 5.00000000e-01],\n",
       "       [9.81818693e-91, 1.96363739e-90, 3.92727477e-90, ...,\n",
       "        2.50000000e-01, 5.00000000e-01, 1.00000000e+00]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.33333333e+000, -6.66666667e-001,  0.00000000e+000, ...,\n",
       "         2.90676725e-106,  1.45338363e-106,  0.00000000e+000],\n",
       "       [-6.66666667e-001,  1.66666667e+000, -6.66666667e-001, ...,\n",
       "         0.00000000e+000,  0.00000000e+000,  0.00000000e+000],\n",
       "       [ 0.00000000e+000, -6.66666667e-001,  1.66666667e+000, ...,\n",
       "         0.00000000e+000,  0.00000000e+000,  0.00000000e+000],\n",
       "       ...,\n",
       "       [ 0.00000000e+000,  0.00000000e+000,  0.00000000e+000, ...,\n",
       "         1.66666667e+000, -6.66666667e-001,  0.00000000e+000],\n",
       "       [ 0.00000000e+000,  0.00000000e+000,  0.00000000e+000, ...,\n",
       "        -6.66666667e-001,  1.66666667e+000, -6.66666667e-001],\n",
       "       [ 0.00000000e+000,  0.00000000e+000,  0.00000000e+000, ...,\n",
       "         0.00000000e+000, -6.66666667e-001,  1.33333333e+000]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.02286385, -0.60023547, -0.43612159, ..., -0.56330828,\n",
       "        -0.263471  ,  0.35776726],\n",
       "       [-0.60023547, 11.278642  , -1.43656359, ..., -0.43187039,\n",
       "         0.06942722,  0.16263109],\n",
       "       [-0.43612159, -1.43656359,  9.16594444, ...,  0.24112757,\n",
       "        -0.20680937,  0.09425723],\n",
       "       ...,\n",
       "       [-0.56330828, -0.43187039,  0.24112757, ...,  4.9541649 ,\n",
       "        -0.22389379, -0.28919304],\n",
       "       [-0.263471  ,  0.06942722, -0.20680937, ..., -0.22389379,\n",
       "         4.57079474, -0.6826814 ],\n",
       "       [ 0.35776726,  0.16263109,  0.09425723, ..., -0.28919304,\n",
       "        -0.6826814 ,  4.19215567]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.dot(C.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15374154,  0.02095593,  0.01417211, ...,  0.02612471,\n",
       "         0.01276738, -0.01190051],\n",
       "       [ 0.02095593,  0.12719178,  0.03471095, ...,  0.01455229,\n",
       "         0.00264128, -0.01148969],\n",
       "       [ 0.01417211,  0.03471095,  0.15644763, ..., -0.01074425,\n",
       "         0.01856873, -0.00813217],\n",
       "       ...,\n",
       "       [ 0.02612471,  0.01455229, -0.01074425, ...,  0.3510296 ,\n",
       "        -0.01226485,  0.0341699 ],\n",
       "       [ 0.01276738,  0.00264128,  0.01856873, ..., -0.01226485,\n",
       "         0.35823919,  0.02833474],\n",
       "       [-0.01190051, -0.01148969, -0.00813217, ...,  0.0341699 ,\n",
       "         0.02833474,  0.45695548]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(C.dot(C.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09071158,  0.38272015,  0.1797117 , ..., -0.22297014,\n",
       "        -0.15155182, -0.02299876],\n",
       "       [ 0.38272015,  0.84337385,  0.54547024, ..., -0.049755  ,\n",
       "        -0.0547505 ,  0.03123735],\n",
       "       [ 0.1797117 ,  0.54547024,  1.13406443, ..., -0.23063027,\n",
       "        -0.29344909, -0.14604015],\n",
       "       ...,\n",
       "       [-0.22297014, -0.049755  , -0.23063027, ...,  1.07034783,\n",
       "         0.55572373,  0.41349008],\n",
       "       [-0.15155182, -0.0547505 , -0.29344909, ...,  0.55572373,\n",
       "         1.34957272,  0.84196009],\n",
       "       [-0.02299876,  0.03123735, -0.14604015, ...,  0.41349008,\n",
       "         0.84196009,  1.4159501 ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_empir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl = GraphicalLasso(assume_centered=True, alpha=0.25, tol=1e-4).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94537365, -0.14709722, -0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.14709722,  1.32796415, -0.33995816, ...,  0.        ,\n",
       "         0.        , -0.        ],\n",
       "       [-0.        , -0.33995816,  1.0498066 , ...,  0.        ,\n",
       "         0.02798588,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  1.00671681,\n",
       "        -0.21531002, -0.02540652],\n",
       "       [ 0.        ,  0.        ,  0.02798588, ..., -0.21531002,\n",
       "         0.97516427, -0.36484121],\n",
       "       [ 0.        , -0.        ,  0.        , ..., -0.02540652,\n",
       "        -0.36484121,  0.89119774]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.5606126388698\n",
      "20.84068729228886\n",
      "13.646749931053467\n",
      "19.961073228773156\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(C.dot(C.T) - np.linalg.inv(c)))\n",
    "print(np.linalg.norm(C.dot(C.T)/10 - np.linalg.inv(c)))\n",
    "print(np.linalg.norm(gl.precision_ - np.linalg.inv(c)))\n",
    "print(np.linalg.norm(np.eye(d) - np.linalg.inv(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.765219425856607\n",
      "8.838883486920599\n",
      "11.518101695447337\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(np.diag(C.dot(C.T)/10) - np.diag(np.linalg.inv(c))))\n",
    "print(np.linalg.norm(np.diag(gl.precision_) - np.diag(np.linalg.inv(c))))\n",
    "print(np.linalg.norm(np.diag(np.eye(d)) - np.diag(np.linalg.inv(c))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94537365 1.32796415 1.0498066  1.34333605 1.08850605 1.19844678\n",
      " 0.93786783 1.14786484 1.3234459  1.09749534]\n",
      "[0.02286385 2.278642   0.16594444 2.17422783 0.76256586 1.41127553\n",
      " 0.27852307 0.87776483 1.73636637 0.95083447]\n",
      "[1.33333333 1.66666667 1.66666667 1.66666667 1.66666667 1.66666667\n",
      " 1.66666667 1.66666667 1.66666667 1.66666667]\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(gl.precision_)[:10])\n",
    "print(np.diag(C.dot(C.T)-9)[:10])\n",
    "print(np.diag(np.linalg.inv(c))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix estimated\n",
      "empirical approximation computed\n"
     ]
    }
   ],
   "source": [
    "from neurips_code.utils import *\n",
    "from neurips_code.predicted_risk import *\n",
    "from neurips_code.Regressor import LinearRegressor\n",
    "\n",
    "c_e = generate_c_empir(X, empir='gl', alpha=0.25)\n",
    "m = np.eye(d)\n",
    "snr = 1\n",
    "include_best_achievable_empirical = True\n",
    "print('covariance matrix estimated')\n",
    "\n",
    "# initialize models\n",
    "reg_2 = LinearRegressor()\n",
    "reg_c = LinearRegressor()\n",
    "reg_ce = LinearRegressor()\n",
    "\n",
    "# generate predictors\n",
    "# matrix specifies which mirror descent we are using (GD if None)\n",
    "reg_2.fit(X, y, matrix = None)\n",
    "reg_c.fit(X, y, matrix = c)\n",
    "reg_ce.fit(X, y, matrix = c_e)\n",
    "\n",
    "w_a = compute_best_achievable_interpolator(X, y, c, m, snr)\n",
    "reg_a = LinearRegressor(init = w_a)\n",
    "\n",
    "if include_best_achievable_empirical:\n",
    "    w_ae = compute_best_achievable_interpolator(X, y, c = c_e, m = np.eye(d), snr = 1, crossval_param = 10)\n",
    "    reg_ae = LinearRegressor(init = w_ae)\n",
    "    print('empirical approximation computed')\n",
    "\n",
    "# best possible linear predictor\n",
    "c_mhalf = np.linalg.inv(scipy.linalg.sqrtm(c)) # inverse square root of the covariance matrix\n",
    "w_b = c_mhalf.dot( np.linalg.lstsq( X.dot(c_mhalf),  xi, rcond=None)[0] ) + w_star # best possible predictor\n",
    "reg_b = LinearRegressor(init = w_b)\n",
    "\n",
    "# calculate the expected risks\n",
    "risk_2 = calculate_risk(w_star, c, reg_2.w ) + sigma2\n",
    "risk_c = calculate_risk(w_star, c, reg_c.w) + sigma2\n",
    "risk_ce = calculate_risk(w_star, c, reg_ce.w) + sigma2\n",
    "risk_a = calculate_risk(w_star, c, reg_a.w) + sigma2\n",
    "if include_best_achievable_empirical:\n",
    "    risk_ae = calculate_risk(w_star, c, reg_ae.w) + sigma2\n",
    "risk_b = calculate_risk(w_star, c, reg_b.w) + sigma2\n",
    "\n",
    "risks = risk_2, risk_c, risk_ce, risk_a, risk_ae, risk_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127.15542471499435,\n",
       " 171.79238056771385,\n",
       " 135.15066024335806,\n",
       " 139.05702715209173,\n",
       " 127.18340808079245,\n",
       " 1.4403526674118354)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix estimated\n",
      "empirical approximation computed\n"
     ]
    }
   ],
   "source": [
    "from neurips_code.utils import *\n",
    "from neurips_code.predicted_risk import *\n",
    "from neurips_code.Regressor import LinearRegressor\n",
    "\n",
    "new_P = C.dot(C.T)\n",
    "m = np.eye(d)\n",
    "snr = 1\n",
    "include_best_achievable_empirical = True\n",
    "print('covariance matrix estimated')\n",
    "\n",
    "# initialize models\n",
    "reg_2 = LinearRegressor()\n",
    "reg_c = LinearRegressor()\n",
    "reg_ce = LinearRegressor()\n",
    "\n",
    "# generate predictors\n",
    "# matrix specifies which mirror descent we are using (GD if None)\n",
    "reg_2.fit(X, y, matrix = None)\n",
    "reg_c.fit(X, y, matrix = c)\n",
    "reg_ce.fit(X, y, matrix = new_P)\n",
    "\n",
    "w_a = compute_best_achievable_interpolator(X, y, c, m, snr)\n",
    "reg_a = LinearRegressor(init = w_a)\n",
    "\n",
    "if include_best_achievable_empirical:\n",
    "    w_ae = compute_best_achievable_interpolator(X, y, c = new_P, m = np.eye(d), snr = 1, crossval_param = 10)\n",
    "    reg_ae = LinearRegressor(init = w_ae)\n",
    "    print('empirical approximation computed')\n",
    "\n",
    "# best possible linear predictor\n",
    "c_mhalf = np.linalg.inv(scipy.linalg.sqrtm(c)) # inverse square root of the covariance matrix\n",
    "w_b = c_mhalf.dot( np.linalg.lstsq( X.dot(c_mhalf),  xi, rcond=None)[0] ) + w_star # best possible predictor\n",
    "reg_b = LinearRegressor(init = w_b)\n",
    "\n",
    "# calculate the expected risks\n",
    "risk_2 = calculate_risk(w_star, c, reg_2.w ) + sigma2\n",
    "risk_c = calculate_risk(w_star, c, reg_c.w) + sigma2\n",
    "risk_ce = calculate_risk(w_star, c, reg_ce.w) + sigma2\n",
    "risk_a = calculate_risk(w_star, c, reg_a.w) + sigma2\n",
    "if include_best_achievable_empirical:\n",
    "    risk_ae = calculate_risk(w_star, c, reg_ae.w) + sigma2\n",
    "risk_b = calculate_risk(w_star, c, reg_b.w) + sigma2\n",
    "\n",
    "risks = risk_2, risk_c, risk_ce, risk_a, risk_ae, risk_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127.15542471499435,\n",
       " 171.79238056771385,\n",
       " 142.45707730526016,\n",
       " 139.05702715209173,\n",
       " 133.7357486844477,\n",
       " 1.4403526674118354)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d8cd6e4c5e5d135b57c7f4afa48b47bbab254644d54890f770c6527a3ab3f54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('preconditioners')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
